% You should title the file with a .tex extension (hw1.tex, for example)
\documentclass[11pt]{article}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=black,
}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{tikz-uml}
\usetikzlibrary{positioning}


\oddsidemargin0cm
\topmargin-2cm     %I recommend adding these three lines to increase the 
\textwidth16.5cm   %amount of usable space on the page (and save trees)
\textheight23.5cm  

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
 
\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\begin{document}

\medskip                        % Skip a "medium" amount of space
                                % (latex determines what medium is)
                                % Also try: \bigskip, \littleskip

\thispagestyle{plain}
\begin{center}                  % Center the following lines
{\Large Emergent property inference python package design} \\
Sean Bittner \\
December 19, 2019 \\
\end{center}

\section{Introduction}
Throughout the emergent property inference (EPI) project \cite{bittner2019interrogating}, we hosted all development code on the cunningham-lab github repo: \href{https://github.com/cunningham-lab/dsn}{dsn} (EPI was formerly DSN), which also relies on the cunningham-lab repo \href{https://github.com/cunningham-lab/tf_util}{tf\_util}, which has customized tensorflow code for normalizing flows, and Hessians of deep generative models.  
Here, we design a new python software package for EPI that is more accessible for our user-base of theoretical neuroscientists. 
While functioning as a software development library for our users with high programming skill, this software shall also run seemlessly on NCAP through a graphical user interface.

\section{Requirements}
EPI is designed to be a tool for theoretical neuroscientists to automate inference in complex models of neural circuits.  For more details, see \cite{bittner2019interrogating}.

The epi python package
\begin{itemize}
\item shall run emergent property inference given
\begin{itemize}
\item a model specification
\begin{itemize}
\item parameter names and continuous ranges
\item emergent property statistics as a differentiable function of parameters
\end{itemize}
\item an emergent property value
\item (optional) a specific normalizing flow architecture 
\item (optional) and specific optimization hyperparameters.
\end{itemize}
\item shall return the distribution sampler upon convergence.
\item shall provide log probabilities, and Hessians throughout the support of the EPI distribution.
\item shall visualize optimization diagnostics.
\item shall visualize the EPI distribution and its properties.
\item shall run on NCAP.
\end{itemize}

\section{Dependency selection}
The EPI method relies on automatic differentiation, so we should use one of the three established open source python packages: PyTorch, Tensorflow, or Jax.
PyTorch has risen in popularity with respect to Tensorflow, possibly due to PyTorch's early implementation and release of a simplifying/intuitive eager execution mode.
The Tensorflow 2.0 release puts these two packages on roughly equal footing.
Jax, a python library for autodiff and efficient optimization with standard python/numpy code, is gaining popularity.

\subsection{Normalizing flows support}
In the end, Tensorflow is the only option with an established normalizing flows API -- one that includes real NVP, MAF, spline flows -- making it the most supportive for our purposes: \href{https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/Bijector}{tf.bijectors}.
PyTorch has a TransformedDistributions class, which doesn't yet support the popular normalizing flow architectures.
PyTorch and Jax have a few options on github, but their level of support is contingent on independent developers (\href{https://github.com/ericjang/nf-jax}{nf-jax}, \href{https://github.com/kamenbliznashki/normalizing_flows}{kamenbliznashki/normalizing\_flows}, \href{https://github.com/karpathy/pytorch-normalizing-flows}{karpathy/pytorch-normalizing-flows}).  If we want to go with these repos, we'll need to have faith in their current implementation, and future maintenance.

\subsection{Model implementation requirement}
The EPI package is designed to receive a differentiable function producing the emergent property statistics from the model parameters.  
In the event that our user-base is largely unfamiliar with Tensorflow/PyTorch, Jax is an interesting alternative, in that we could lower the barriers-to-entry for our software package.
Specifically, users could submit native python functions through which Jax can take gradients.
Otherwise, we must require our users to write Tensorflow or PyTorch functions, or find a reliable way of taking Tensorflow or PyTorch gradients through native python.

To gain an idea of the programming experience of our userbase, I asked 20 Theory Center members working on theoretical modeling projects whether they a.) used python and b.) if so, knew PyTorch or Tensorflow.  The results suggest a 70\% level of familiarity with at least one of PyTorch or Tensorflow.

\begin{table}[h]
\caption{Theoretical neuroscientist programming experience}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
no python & python (no autograd) & python autograd\\ \hline
3 & 3 & 14 \\ \hline
\end{tabular}
\end{center}
\end{table}
\subsection{Tensorflow implementation rationale}
The lack of a well-supported normalizing flows library for PyTorch and Jax makes these option unattractive.  Given the relatively high python autograd proficiency of our user-base, I conclude that it makes sense to design the package to accept a Tensorflow function defining the emergent property statistics.

\clearpage
\section{EPI package design}
EPI is a method designed to run for a broad space of theoretical models and behaviors.  EPI's implementation changes according to the details of each model.  It makes sense to have our users construct a model object (defined by the Model class), which has a set of defined parameters and an eps() (emergent property statistics) function.  With these class members, we can then run the method epi(), which will return an optimized distribution object (Distribution class).

We create an Architecture class as well, to shield the user from Tensorflow, except through the supplied eps() function.  
As a result, the user can construct simplified python objects (constructors of these classes) according to their use case, run the EPI method, and have a simplified python distribution object returned.
The normalizing flows library and autograd functionality of Tensorflow are used by the package, yet not exposed to the user.

\subsection{EPI class diagram}
\begin{tikzpicture} 
\begin{umlpackage}{epi}
\umlclass{Model}{ 
  + name : string \\ 
  + parameters : list of parameters \\ 
  }
  { 
  \# \_set\_name() \\
  \# \_set\_parameters() \\
  + set\_eps() \\
  + eps() \\
  + epi() \\
  + load\_epi\_dist() \\
}

\umlclass[x=7, y=0]{Parameter}{ 
  + name : string \\ 
  + bounds : [float, float] \\ 
  }
  { 
  \# \_set\_name() \\
  \# \_set\_bounds() \\
} 

\umlclass[x=0, y=-6]{Architecture}{ 
  + arch\_type : string \\ 
  + D : pos int \\ 
  + num\_stages :  int \\ 
  + num\_units :  int \\ 
  + num\_layers :  int \\
  + batch\_norm : bool \\
  + bn\_momentum : float \\ 
  + post\_affine : bool \\
  + bounds : tuple \\
  + random\_seed : int \\
  }
  { 
  \# setters
  + sample(N : int) \\
  + initialize(init\_type, init\_params) \\
  + to\_string() \\
} 


\umlclass[x=6, y=-6]{Distribution}{ 
  \# \_D : int \\ 
  \# \_parameters : list of parameters \\ 
  }
  { 
  + sample() \\
  + log\_prob() \\
  + grad() \\
  + hessian() \\
}

\umlunicompo{Model}{Parameter}
\umlunicompo{Distribution}{Parameter}
\umldep{Model}{Architecture}
\umldep{Model}{Distribution}
\end{umlpackage}

\end{tikzpicture}

\subsection{Key functions}
\begin{verbatim}
Model.eps(): parameters (list of tf.Tensors) -> T_x (tf.Tensor)
\end{verbatim}
Tensorflow computation of the the emergent property statistics (eps) of the model  from an arbitrary length list of parameters.  The user can provide this function during initialization or through the Model.set\_eps() method.

\begin{verbatim}
Model.epi(): mu (np.array), arch (Architecture), hps, ... -> 
             epi_dist (Distribution)
\end{verbatim}
Run EPI on the model (which has set parameters and an Tensorflow eps() function) for emergent property value ``mu" with optional parameters for architecture ``arch" and hyperparameters ``hps".

\begin{verbatim}
Architecture.to_model(): self -> tf.Model
\end{verbatim}
Convert Architecture class to a realization of a Tensorflow normalizing flow distribution with the given architecture.

\section{Testing}
\subsection{Unit testing}
We will unit test the API functionality with pytest and codecov.  Package stability on multiple OS's and builds will be assessed with Travis CI, running these unit tests and coverage diagnostics.

\subsection{Validation}
Algorithm validation can happen be done on exponential families with known solutions, and simple systems with derivable contours like the oscillating 2D linear dynamical system.

\section{Documentation}
We will use ReadTheDocs for automatic documentation generation and hosting (no gitpage).

\section{Style}
We will use black as our automatic style formatter.

\bibliography{epi_software_docs}
\bibliographystyle{unsrt}
\end{document}

