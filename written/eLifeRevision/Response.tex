%--------------------
% Packages
% -------------------
\documentclass[11pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{gentium}


\usepackage[pdftex]{graphicx} % Required for including pictures
\usepackage[swedish]{babel} % Swedish translations
\usepackage{calc} % To reset the counter in the document after title page
\usepackage{enumitem} % Includes lists

\frenchspacing % No double spacing between sentences
\linespread{1.2} % Set linespace
\usepackage[a4paper, lmargin=1in, rmargin=1in, tmargin=1in, bmargin=1in]{geometry} %margins
%\usepackage{parskip}

\usepackage[all]{nowidow} % Tries to remove widows
\usepackage[protrusion=true,expansion=true]{microtype} % Improves typography, load after fontpackage is selected

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{bm}
\usepackage{xr}
\externaldocument{Bittner_eLife2021}
\usepackage{xcite}
\externalcitedocument{Bittner_eLife2021}
\usepackage{qting}
\quotefrom{Bittner_eLife2021}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

%----------------------- 
% Begin document
%-----------------------
\begin{document} 

{\Large \textbf{Response to reviewers}} \\
(original comments in bold)

\textbf{Bittner and colleagues introduce a machine learning framework for maximum entropy inference of model parameter distributions that are consistent with certain emergent model properties, specified by the investigator. The approach is illustrated on several models of potential interest.} 

\textbf{Reviewers were broadly enthusiastic about the potential usefulness of this methodology. However, the reviews and ensuing discussion revealed several points of concern about the manuscript and the approach. The full reviewer comments are included below.} 

We thank the reviewers for the excellent and constructive feedback.  The reviews contained broad support for this work, but with significant calls for improvements, especially around clarity of exposition and the thoroughness of the evaluation (comparison to existing methods, more depth in scientific analyses, etc).  We strongly agreed and thus committed a significant period of time to improving these areas.  List of key manuscript improvements:
\begin{itemize}
\item We completely overhauled our presentation of the parameter inference technique emergent property inference (EPI). See response to main concern \#1, especially noted by R1 and R3.
\item We added an entirely new results section to compare EPI to alternative parameter inference techniques (SMC-ABC and SNPE) as parameter count increases, which demonstrates the advantages of EPI in high dimensional parameter spaces. See response to main concern \#2, especially noted by R2 and R3.
\item EPI is used to understand how noise across neuron type populations governs excitatory variability in a model of primary visual cortex.  Our model analyses in general now focus on analyzing the structure of parameter distributions, which EPI captures using flexible approximations enabled by deep learning.  
Here, EPI yields novel insight where the conventional analytic approach became infeasible with increasing neuron types.  See response to main concern \#4.
\item EPI is used to discover multiple parametric regimes of rapid task switching in a model of superior colliculus.  
These regimes are efficiently identified and characterized using the probabilistic modeling tools afforded by EPI, and we relate the inferred connectivities to results from optogenetic silencing experiments in rats. See response to main concern \#4.
\end{itemize}

\textbf{The main concerns are summarized as follows:}

\textbf{1. The methodology is not adequately explained. Both the body text and methods section present a somewhat selective description that is very hard to follow in places and should be checked and rewritten for clarity, completeness, notational consistency and correctness.}

Agreed.
In the introduction, we present our method by focusing on the key aspect of EPI that differentiates it from other approaches to inverse problems.
Specifically, we explain how current methodology infers the parameters producing computation by conditioning on \textit{exemplar datasets} (real or simulated), whereas in EPI we condition directly on the \textit{emergent properties} that define the computation.  We also note (and have an appendix detailing) how EPI is in fact doing variational bayesian inference, but that the parameterization of the problem as we solve it is more natural given the goal of reproducing a computation.  In other words, the methodology is mathematically sound and directly connected to well-worn techniques, but the specific form we solve is the appropriate choice for the motivating problem.

\begin{displayquote}
Lines 44-52\\
``\quoting{intro_data_ep}''
\end{displayquote}

We now frame the method within the more general context of parameter inference techniques in neuroscience, rather than the context of recent advancements in machine learning.
We have made concentrated efforts to simplify language and to relate to all relevant existing methodology.

\begin{displayquote}
Lines 53-63\\
``\quoting{intro_epi}''
\end{displayquote}

To improve clarity, we have overhauled and simplified the notation and presentation of emergent properties.
Emergent properties are now denoted with $\mathcal{X}$: rather than a vector of first- and second-moment constraints, we present the emergent property more readably (and equivalently) as mean and variance constraints on emergent property statistics:

\begin{displayquote}
Line 742 \\
``
\begin{equation} \tag{12}
\mathcal{X}:~ \mathbb{E}_{\mathbf{z},\mathbf{x}}\left[f(\mathbf{x}; \mathbf{z})\right] = \bm{\mu},~~ \text{Var}_{\mathbf{z},\mathbf{x}}\left[f(\mathbf{x}; \mathbf{z})\right] = \bm{\sigma}^2
\end{equation}''
\end{displayquote}

Emergent properties and EPI are introduced and explained with this revised notation in Section \ref{results_dgm}, and we have completely redone the Methods Section \ref{methods_EPI} to improve clarity, precisely explain EPI optimization (Section \ref{methods_AL_opt}), and derive equivalence to established bayesian inference techniques (Section \ref{methods_VI}).

In our revised writing, we deemphasize the role of maximum entropy in the EPI algorithm, because this has largely served as a distraction in our experience.
We show in Section \ref{methods_VI} that EPI is exactly variational inference (this was mentioned in the previous draft but not as a point of connection and focus, as it should be and now is).   Therefore, it makes sense to present EPI in the main text as a statistical inference technique that constrains the predictions of the inferred parameters to be an emergent property, and leave the details of the maximum entropy to the technically proficient in Section \ref{methods_EPI}.

\begin{displayquote}
Lines 152-154\\
``\quoting{results_ME}''
\end{displayquote}

Figure \ref{fig:STG} has been largely redone to reflect the modified presentation, and improve the pictorial representation of the method.
In Section \ref{results_RNN}, we now show that EPI is the only simulation-based inference method that controls the predictions of its inferred distribution (Figure \ref{fig:RNN}C-D).

Finally, we emphasize the utility of this deep inference technique for scientific inquiry in a new paragraph at the end of Section \ref{results_dgm}.

\begin{displayquote}
Lines 159-175\\
``\quoting{results_hess}''
\end{displayquote}

\textbf{2. The computational resources required to use this method are not adequately benchmarked. For example, the cosubmission (Macke et al) reported wall clock time, required hardware and iterations required to produce results by directly comparing to existing methods (approximate bayesian computation, naive sampling, etc.) Without transparent benchmarks it is not possible to assess the advance offered by this method.} 

We thank the reviewers for emphasizing the importance of this methodological comparison.  In Section 3.3, we provide a direct comparison of EPI to alternative simulation-based inference techniques SMC-ABC and SNPE by inferring RNN connectivities that exhibit stable amplification.  These comparisons evaluate both wall time (Figure 2A) and simulation count (Figure 2B), and we explain how each algorithm was run in its preferred hardware setting in Section 5.3.4.
 
In this analysis, we demonstrate the improved scalability of deep inference techniques (EPI and SNPE) with respect to the state-of-the-art approximate bayesian computation technique (SMC-ABC).
While controlling for architecture size (Figure 2-figure supplement 1), we push the limits of SNPE through targeted hyperparameter modifications (Figure 2-figure supplement 2), and show that EPI scales to higher dimensional RNN connectivities producing stable amplification than SNPE.
Furthermore, we emphasize that SNPE does not constrain the properties of the inferred parameters; many connectivities inferred by SNPE result in unstable or nonamplified models (Figure 2C, Figure 2-figure supplement 3).

\textbf{3. The extent to which this method is generally/straightforwardly applicable was in doubt. It seemed as though a significant amount of computation was required to do inference on one specified property and that the computation would need to be run afresh to query a new property. The methodology in the cosubmission (Macke) made clear that computation required for successive inferences is 'amortized' during training on random parameters. Moreover, EPI seemed less flexible than the cosubmission's approach in that it required a differentiable loss function. The complementarity and advantages of this approach as opposed to the cosubmission's approach are therefore unclear. } 

We respectfully disagree with this characterization, but we take responsibility here: we certainly needed to improve our exposition and comparisons to clarify the general applicability of EPI.  We have done so, and what results is a meaningful comparison with the cosubmission (SNPE, Macke et al, now published) and other methods.  Of course, there is a tradeoff (no free lunch, as usual), and these new analyses clarify when EPI is preferable to SNPE, and vice versa.  

First, unlike SNPE, EPI leverages gradients of the emergent property throughout optimization, which leads to better efficiency and scalability (Section 3.3).   This is the usual tradeoff of gradient-based vs sampling-based methods.  The emergent properties of many models in neuroscience are tractably differentiable, four of which we analyze in this manuscript ranging across levels of biological realism, network size, and computational function. This tradeoff is now explained in the Discussion (Section \ref{discussion}).

Second, of course the reviewers are right to point out EPI does not amortize across emergent properties, and it requires differentiability of the emergent properties of the model.  SNPE is more suitable for inference with nondifferentiable mechanistic models and scientific problems requiring many inferred parameter distributions.  
However, these relative drawbacks of EPI with respect to SNPE can be considered choices made in a tradeoff between simulation-based inference approaches.  

On the balance, these two methods occupy different areas of use, and both are equivalently ``generally/straightforwardly applicable''; we believe the new analyses and discussion in the manuscript clarify that, as it is a key point for practitioners going forward.   

\begin{displayquote}
Lines 435-447\\
``\quoting{discussion_gradients}''
\end{displayquote}

Furthermore, EPI focuses the entire expressivity of the approximating deep probability distribution on a single distribution, rather than spreading this expressivity to some uncharacterized degree across the chosen training distribution of amortized posteriors in SNPE (Section 5.1.1 Related Approaches).

\begin{displayquote}
Lines 832-838\\
``\quoting{amortization}''
\end{displayquote}
 
Finally, we emphasize that EPI does something fundamental that SNPE and other inference techniques cannot.  EPI learns parameter distributions whose predictions are constrained to produce the emergent property.  We show in Figure 2 and Supplementary Figure 2-figure supplement 3 that SNPE does not control the statistical properties of its predictions, resulting in the inference of many parameters that are not consistent with the desired emergent property.

\begin{displayquote}
Lines 228-238\\
``\quoting{rnn_constrained}''
\end{displayquote}

\textbf{4. Some examples lack depth in their treatment (see reviewer comments) and in some cases the presentation is somewhat misleading. The STG example is not in fact a model of the STG. The cosubmission (Macke) uses a model close to the original Prinz et al model, which is a model of the pyloric subnetwork. It would be instructive to benchmark against this same model, including computation time/resources required. Secondly, the subsequent example (input-responsivity in a nonlinear sensory system) appeared to imply that EPI permits 'generation' and testing of hypotheses in a way that other methods do not. All the method really does is estimate a joint distribution of feasible parameters in a specific model which is manually inspected to propose hypotheses. Any other method (including brute force sampling) could be used in a similar way, so any claim that this is an integral advantage of EPI would be spurious. Indeed, one reviewer was confused about the origin of these hypotheses. While it is helpful to illustrate how EPI (and other methods) could be used, the writing needs to be far clearer in general and should clarify that EPI does not offer any new specific means of generating or evaluating hypotheses.}

We thank the reviewers for explaining how they found some of the
presentation misleading.  We have taken serious care in this manuscript to clarify a.)
what is novel, appreciable scientific insight provided by EPI, as well as b.) which scientific
analyses are made possible by EPI.

\begin{enumerate}[label=(\alph*)]
\item  In the revised manuscript we clarify that novel theoretical insights are not being made into the STG subcircuit model or the recurrent neural network models.  The STG subcircuit serves as a motivational example to explain how EPI works, and we use RNNs exhibiting stable amplification as a substrate for scalability analyses.  

\begin{displayquote}
Lines 75-79 \\
``\quoting{intro_results1}''
\end{displayquote}

We do produce strong theoretical insights into a model of primary visual cortex (Section 3.4) and superior colliculus (Section 3.5).  These analyses have substantially more depth than the previous manuscript.

\begin{displayquote}
Lines 79-87 \\
``\quoting{intro_results2}''
\end{displayquote}

\item The ability to infer a flexible approximation to a probability distribution constrained to produce an emergent property is novel in its own right (Figure 2).  The deep probability distribution fit by EPI facilitates the mode identification (via gradient ascent of the parameter log probability) and sensitivity measurement (via the measurement of the eigenvector of the Hessian at a parameter value). These mode identifications and sensitivity measurements are done in Sections 3.1 (Figure 1E), 3.4 (Figure 3E), and 3.5 (Figure 4C).  By using this mode identification technique along the ridges of high parameter probability in the SC model, we identify the parameters transitioning between the two regimes.  Finally, the sensitivity dimensions of the SC model identified by EPI facilitated regime characterization through perturbation analyses (Figure 4D).
 
\end{enumerate}

Importantly, we do not claim that these theoretical insights were necessarily dependent on using the techniques in b.).  
One could have come to these conclusions via various combinations of techniques mentioned in Section 5.1.1 Related Methods.
In the case of the V1 model inference, the main point is to indicate that such insight can be afforded by EPI and its related methods, in contrast to the analytic derivations emblematic of practice in theoretical neuroscience.
\begin{displayquote}
Lines 297-305 \\
``\quoting{v1_summary}''
\end{displayquote}
In the case of the SC model inference, random sampling would have taken prohibitively long, and it is unclear how the continuum between the two connectivity regimes would have been identified with alternative techniques:
\begin{displayquote}
Lines 412-415 \\
``\quoting{sc_summary}''
\end{displayquote}
 
As the reviewers indicate, the STG model analyzed in our manuscript is not that of Prinz et al. 2004, and thus not the model analyzed by the cosubmission (Macke et al).  We chose an alternative model, believing it important to demonstrate inference in biophysically realistic Morris-Lecar models when gradients are tractable, which is the case of the 5-neuron STG model we analyzed from Gutierrez et al. 2013.  This 5-neuron model represents the IC neuron (hub) and its coupling to the pyloric (fast) or gastric mill (slow) subcircuit rhythms.  In the introductory text, we refer to this model as the “STG subcircuit” model (rather than “STG model”), and we better clarify what aspect of the STG is being modeled in Results Section 3.1. 

\begin{displayquote}
Lines 98-103 \\
``\quoting{stg_subcircuit}''
\end{displayquote}

The difference between this model and the STG model of the pyloric subnetwork is emphasized in Discussion:

% Within another quote so need to copy.
\begin{displayquote}
Lines 440-443 \\
``However, conditioning on the pyloric rhythm \cite{marder1992dynamic} in a model of the pyloric subnetwork model \cite{prinz2004similar} proved to be prohibitive with EPI.
The pyloric subnetwork requires many time steps for simulation and many key emergent property statistics (e.g. burst duration and phase gap) are not calculable or easily approximated with differentiable functions.
''
\end{displayquote}

\textbf{5. There is a substantial literature on parameter sensitivity analysis and inference in systems biology, applied dynamical systems and control that has been neglected in this manuscript. The manuscript needs to acknowledge, draw parallels and explain distinctions from current methods (ABC, profile likelihood, deep learning approaches, gaussian processes, etc). The under-referencing of this literature deepened concerns about whether this approach represented an advance. DOIs for a small subset of potentially relevant papers include:}

\textbf{https://doi.org/10.1038/nprot.2014.025 \\
http://doi.org/10.1085/jgp.201311116 \\
http://doi.org/10.1016/j.tcs.2008.07.005 \\
http://doi.org/10.3182/20120711-3-BE-2027.00381 \\
http://doi.org/10.1093/bioinformatics/btm382 \\
http://doi.org/10.1111/j.1467-9868.2010.00765.x \\
http://doi.org/10.1098/rsfs.2011.0051 \\
https://doi.org/10.1098/rsfs.2011.0047 \\
http://doi.org/10.1214/16-BA1017 \\
https://doi.org/10.1039/C0MB00107D}

Thank you for pointing us to these references on sensitivity analyses, MCMC inference, and applied dynamical systems.  
We have incorporated most of them into the current manuscript and explain EPI’s relation to each class of these techniques throughout Section \ref{methods_related} Related approaches.

\textbf{6. One of the reviewers expressed concern that the work might have had significant input from a senior colleague during its early stages, and that that it might be worth discussing with the senior colleague whether their contribution was worthy of authorship. The authors may take this concern into account in revising the manuscript.}

Thank you for pointing this out.  We take authorship and scientific contribution very seriously (it is always part of our manuscript submission process, as it was here).  We have gone back and discussed every researcher who was in any way involved in this work, and to our best estimation, we think this comment references Woods Hole Course Project mentors where this work was discussed and explored in its early stages.   We have had explicit followup conversations with James Fitzgerald and Dhruva Raman and noted that of course we would be happy to have them involved at any level (great colleagues!).  They both reported they are happy with their acknowledgement in the paper and don't feel that there is a justification for authorship (again we were very welcoming of this possibility and maintain positive enthusiasm for both).  Furthermore, Stephen Baccus has requested that we acknowledge the summer course, which we have done.  If there is any other ``senior colleague'' the reviewer had in mind, please let us know and we will of course gladly pursue the matter. 

\textbf{7. Finally, please also address specific points raised by the reviewers, included below. } \\

{\Large \textbf{Reviewer \#1: }}

\textbf{General Assessment: The authors introduce a machine learning framework for maximum entropy inference of model parameters which are consistent with certain emergent properties, which they call 'emergent property inference'. I think this is an interesting and direction, and this looks like a useful step towards this program. I think the paper could be improved with a more thorough discussion both of the broad principles their black box approach seeks to optimize, as well as the details of its implementation. I also think the detailed examples should be more self-contained. Finally I find this work to be somewhat misrepresented as a key to all of theoretical neuroscience. This approach may have some things to offer to the interesting problem of finding parameter regions of models, but this is not the entirety of, nor really a major part of theoretical neuroscience as I see it. }

We thank the reviewer for their positive comments and thoughtful feedback.
We have made serious effort to improve our presentation and explanation of EPI (see response to main concern \#1).
Furthermore, we have focused on clearly motivating and describing each neural circuit model studied in this manuscript.
Sufficient mathematical detail is written in each results section, while full details are presented in Methods.
All code for training EPI on these models and their analysis are available in well-documented scripts and notebooks in our github repository.
\begin{verbatim} https://github.com/cunningham-lab/epi \end{verbatim}

We modify our writing to clarify that EPI is not a key to all of theoretical neuroscience, but rather a powerful solution to inverse problems in neural circuit modeling.
Inverse problems are indeed a major part of theoretical neuroscience, and their solutions are important for the evaluation  of neural circuit models.
We clarify this point in the introduction:

\begin{displayquote}
Lines 26-35 \\
``\quoting{intro_first}''
\end{displayquote}

\textbf{Other concerns: }

\textbf{(1) Maximizing the entropy of the distribution is not a reparameterization invariant exercise. That is, results depend on whether the model parameters contains rates or time constants, for example. I wonder if this approach is attempting to use a 'flat prior' in some sense which has the same reparameterization issue? Can the authors comment? }

The reviewer is correct to point out that maximum entropy solutions are not reparameterization invariant, and indeed the units matter.  The reviewer’s suggestion that the method is in some sense using a flat prior is also correct.  To clarify, EPI does not execute posterior inference, because there is no explicit dataset or specified prior belief in the EPI framework.  
However, we derive the relation of EPI to bayesian variational inference in Section \ref{methods_VI}, which shows EPI uses a uniform prior when framed as variational inference.

\begin{displayquote}
Lines 1027-1031 \\
``\quoting{EPI_VI}''
\end{displayquote}
 
In our examples, we only infer distributions of parameters with the same units, so this issue should not draw concern over the validity of our model analyses.  As suggested, the EPI solution will differ according to relative scaling of parameter values under the maximum entropy selection principle. Thus, an important clarification is that sensitivity quantifications are made in the context of the chosen parameter scalings. 

\textbf{(2) I don't think this is a criticism of the work, but instead of the writing about it: I find the introductory paragraphs to give a rather limited overview of theory as finding parameters of models which contain the right phenomenology. }

We appreciate this feedback.  We have adapted the introduction to clarify that we are focusing on solving inverse problems in theoretical neuroscience.

\textbf{(3) I am somewhat familiar with the stomatogastric circuit model, and so that is where I think I understand what they have done best. I don't understand what I should take away from their paper with regards to this model. Are there any findings that hadn't been appreciated before? What does this method tell us about the system and or its model? }

We clarify in point 4 above that we are not claiming to produce novel, appreciable scientific insight about the STG subcircuit model, which is used as a motivation example. The takeaway is that the conductance parameters producing intermediate hub frequency belong to a complex 2-D distribution, which EPI captures accurately, and that EPI can tell us the parameter changes away from the prototypical configuration that change hub frequency the most or least.  For example, for increases in $g_{\text{el}}$ and $g_{\text{synA}}$ according to the proportions of the degenerate dimension of parameter space, intermediate hub neuron frequency will be preserved in this model.  This suggests that in the real STG neural circuit, the IC neuron will remain at an intermediate frequency between the pyloric and gastric mill rhythms if parameter changes are made along such a dimension.

\begin{displayquote}
Lines 171 -173 \\
``The directionality of $\mathbf{v}_2$ suggests that changes in conductance along this parameter combination will most preserve hub neuron firing between the intrinsic rates of the pyloric and gastric mill rhythms.''
\end{displayquote}
 
\textbf{(4) I don't follow the other examples. Ideally more details should be given so that readers like myself who don't already know these systems can understand what's been done. }

Thank you for the feedback.  We have taken care to give more general context and motivation for each neural circuit model.

\textbf{(5) In figure 2C, the difference between the confidence interval between linear and nonlinear predictions is huge! How much of this is due to nonlinearities, and how much is due to differences in the way these models are being evaluated? }

In the current manuscript, we do not examine the difference between linear and nonlinear predictions of the V1 model. \\

{\Large \textbf{Reviewer \#2: }}

\textbf{General assessment }

\textbf{This is a very interesting approach to an extremely important question in theoretical neuroscience, and the mathematics and algorithms appear to be very rigorous. The complexities in using this in practice make me wonder if it will find wide application though: setting up the objective to be differentiable, tweaking hyperparameters for training, and interpreting the results; all seem to require a lot from the user. On the other hand, the authors are to be congratulated on providing high quality open source code including clear tutorials on how to use it. }

We thank the reviewer for their positive comments.  

\textbf{Major points }

\textbf{1. Training deep networks is hard. Indeed the authors devote a substantial amount of the manuscript to techniques for training them, and note that different hyperparameters were necessary for each of the different studies. Can the authors be confident that they have found the network which gives maximum entropy or close to it? If so, how. If not, how does that affect the conclusions? }

We agree with the reviewer that hyperparameter sensitivity and global optima are important considerations of any optimization algorithm using deep neural networks.  To draw a parallel, training deep networks for visual processing used to be considered infeasible, but became easier through iterative improvements in architectural and hyperparameter choices that spread across the field.  
Similarly, training deep networks via EPI to learn parameter distributions became easier throughout this research project as we learned through trial and error what works well.  
In fact, there has been extraordinary progress in the field of deep probability distributions (specifically normalizing flows), that have allowed EPI to converge regularly while capturing complex structure (e.g. Dinh et al. 2017 and Kingma et al. 2018).  
This manuscript's explanation of hyperparameter choices and the extensive set of examples in the online code will serve as valuable guidelines for future research using this method.
Every figure of this paper is reproducible with the jupyter notebooks, and there are several tutorials for understanding the most consequential hyperparameters: the augmented lagrangian constant and deep probability distribution architecture.  

In general, we cannot know if we have obtained the global maximum entropy distribution for a given emergent property.  
The reviewer is correct to point out that multiple distributions may satisfy the emergent property and have different levels of entropy.  
In the new manuscript, we present the method as an inference technique without focusing very greatly on maximum entropy, since it tends to distract and confuse the reader.  
We derive an analytic equivalence to variational inference (Section 5.1.6) showing that a.) EPI is a valid inference method, and b.) to emphasize that maximum entropy is the normative selection principle of bayesian inference methods in general. 
Thus, the concern of not having the globally optimal inferred distribution is the same that applies to all other statistical inference techniques.

Practically, this has scientific implications.  
It means that we may be missing important structure in the inferred distribution, or we may be missing additional modes in parameter space.  
To handle this methodologically, we run EPI with multiple random seeds, and select the distribution that has converged with the greatest entropy for scientific analysis.
Throughout the manuscript, we compare to analytic, error contour, and brute-force ground truth to ensure we are capturing the correct distribution with EPI (see response to R3 concern 1).

\textbf{2. Interpreting the results still seems to require quite a lot of work. For example, from inspecting Fig 2 the authors extract four hypotheses. Why these four? Are there other hypotheses that could be extracted and if not how do we know there aren't? Could something systematic be said here? }

This analysis is no longer in the manuscript.

\textbf{3. Scalability. The authors state that the method should in principle be scalable, but does that apply to interpreting the results? For example, for the V1 model it seems that you need to look at 48 figures for 4 variables, and I believe this would scale as O(n$^2$) with n variables. This seems to require an unsustainable amount of manual work?}

We refer the reviewer to Figure 2 and Section 3.3 for scalability analysis.  The scaling analysis addresses the question of the issue of parameter discovery with EPI in high-dimensional parameter spaces.

Another important question the reviewer brings up is how well one can analyze the high-dimensional parameter distributions that EPI produces?  
Indeed, these distributions become more challenging to understand and visualize in high dimensions.  
This is where the sensitivity measurements appearing in sections 3.1, 3.4, and 3.5 can be particularly useful.  
Even in high dimensions, trained deep probability distributions offer tractable quantitative assessments of how parametric combinations affect the emergent property that was conditioned upon.

\textbf{4. There are some very particular choices made in the applications and I wonder how general the conclusions are as a consequence. For example, in equation (5) the authors choose an arbitrary amount of variance 0.01$^2$ - why? In the same example, why look at y=0.1 and 0.5? }

In the current manuscript, we make sure to explain all choices of the emergent property constraints  Here, we show the description of each emergent property with equations omitted.

\begin{displayquote}
Section \ref{results_dgm}, Lines 136-140\\
``\quoting{stg_ep_text1}~\quoting{stg_ep_text2}.''
\end{displayquote}

\begin{displayquote}
Section \ref{results_RNN}, Lines 200-206 \\
``\quoting{rnn_ep_text1}~... \quoting{rnn_ep_text2}~\quoting{rnn_ep_text3}''
\end{displayquote}

\begin{displayquote}
Section \ref{results_V1}, Lines 275-278\\
``\quoting{v1_ep_text1}~\quoting{v1_ep_text2}''
\end{displayquote}

\begin{displayquote}
Section \ref{results_SC}, Lines 331-334\\
``\quoting{sc_ep_text1}.~\quoting{sc_ep_text2}''
\end{displayquote}
\textbf{Minor points }

\textbf{The introduction and discussion are very clearly written but the results section is hard going. Partly this is unavoidable given the subject matter, but a few sentences here and there might help the reader along. Things like "x is the internal state of the model, z are the parameters we will change, ...". When introducing entropy in equation (3), H isn't previously defined, and again it might help to give the reader a hand here, e.g. "max entropy means the distribution is as spread out as possible" (you can surely find a better thing to say than this, but just to give an idea). The other point which is quite hard to follow is interpreting e.g. Fig 2C. Perhaps for Hypothesis 1 you could write a couple of sentences explaining slightly more clearly why seeing small blobs or horizontal/vertical lines in these distribution plots means that it's mainly determined by the direct input. }

Thank you for the detailed suggestions on how to improve writing in the results sections.
 We have taken the specific suggestion of explicitly calling out $\mathbf{x}$ as network state, and we have moved complicated discussion of the role of entropy to the Methods section.

{\Large \textbf{Reviewer \#3:}}

\textbf{This paper addresses a major issue in fitting neuroscience models: how to identify the often degenerate, or nearly degenerate, set of parameters that can underlie a set of experimental observations. Whereas previous techniques often depended upon brute force explorations or special parametric forms or local linearizations to generate sets of parameters consistent with measured properties, the authors take advantage of deep generative networks to address this problem. Overall, I think this paper and the complementary submission have the potential to be transformative contributions to model fitting efforts in neuroscience. That being said, since the primary contribution is the methodology, I think the paper requires more systematic comparisons to ground truth examples to demonstrate potential strengths and weaknesses, and more focus on methodology rather than applications. }

We thank the reviewer for their positive comments and thoughtful feedback.
We have made great efforts to provide several clear comparisons to ground truth (see response to concern \#1), and now provide an extensive methodological comparison to SMC-ABC and SNPE (see response to Main concern \#2).

\textbf{Substantive concerns: \\
1) The authors only have a single ground-truth example where they compare to a known result (a 2x2 linear dynamical system). It would be good to show how well this method compares to results from, for example, a direct brute force grid search of a system with a strongly non-elliptical (e.g. sharply bent) shaped parameter regime and a reasonably large (e.g. 5?) number of parameters corresponding to a particular property, to see how well the derived probability distribution overlaps the brute force grid search parameters (perhaps shown via several 2-D projections). }

We thank the reviewer for pointing out the importance of ground truth comparisons in this manuscript.
In this revision, we make ground truth comparisons via analytic derivations, empirical error contours, and brute-force sampling.

\textit{Analytic comparisons}:
The 2x2 linear dynamical system is chosen as a worked example because it has multi-modal non-elliptical structure (Figure 1-figure supplement 1), and its contours can be derived analytically (Figure 1-figure supplement 2).
Similarly, in Section \ref{methods_V1_complexity}, we derive the quadratic relationship between excitatory variability and input noise variability (in a simplified model) suggesting that the quadratic relationship uncovered by EPI (see Section \ref{results_V1}) is correct.

\textit{Error contours}:
In the motivation example, we compare the EPI inferred distribution of STG conductances to hub frequency contours (Figure \ref{fig:STG}E), which show that the non-elliptical parametric structure captured by EPI is in agreement with these contours.  
This general region of parameter space was labeled following grid search analyses in a previous study (Gutierrez et al. 2013, Figure 2, parameter regime G).

\textit{Brute-force}:
The EPI inferred distribution for rapid task switching in the SC model is sharply bent (Figure \ref{fig:SC}), and matches the parameter set returned from random sampling (Figure 4-figure supplement 5A).  
We note that the brute-force parameter set is actually not the ground-truth solution, because it does not obey the constraints of the emergent property as the EPI distribution does (Figure 4-figure supplement 5B).
This can explain the spurious samples in the brute-force set that are not in the EPI inferred distribution.

All EPI distributions shown in this manuscript are “validated” in the sense that they pass a hypothesis testing criteria for emergent property convergence; all EPI distributions produce their emergent properties.
Finally, the underlying maximum entropy flow network (MEFN) algorithm is compared to a ground truth solution (Loaiza Ganem et al. 2017, Figure 2) by deriving ground truth from the duality of maximum entropy distributions and exponential families (see Section \ref{methods_ME_EF}).

\textbf{2) It was not obvious whether EPI actually scales well to higher dimensions and how much computation it would take (there is one claim that it 'should scale reasonably'). While I agree that examples with a small number of parameters is nice for illustration, a major issue is how to develop techniques that can handle large numbers of parameters (brute force, while inelegant, inefficient, and not producing an explicit probability distribution can do a reasonable job for small \#'s of parameters). The authors should show some example of extending to larger number of parameters and do some checks to show that it appears to work. As a methodological contribution, the authors should also give some sense of how computationally intensive the method is and some sense of how it scales with size. This seems particularly relevant to, for example, trying to infer uncertainties in a large weight matrix or a non-parametric description of spatial or temporal responses or a sensory neuron (which I'm assuming this technique is not appropriate for? See point\#4 below). }

The reviewer is right to point out the importance of a scaling analysis. Please see response to Main concern \#2.

\textbf{3) For the STG-like example, this was done for a very simple model that was motivated by the STG but isn't based on experimental recordings. Most of the brute force models of the STG seek to fit various waveform properties of neurons and relative phases. Could the model handle these types of analyses, or would it run into problems due to either needing to specify too many properties or because properties like "number of spikes per burst'' are discrete rather than continuous? This isn't fatal, but would be good to consider and/or note explicitly. }

The STG subcircuit model of Gutierrez et al. 2013 is certainly less complex than other models of the STG, yet 5 connected Morris-Lecar neurons is certainly a nontrivial system.
We clarify why this model is analyzed in Section \ref{results_motivating} instead of more complex STG models when discussing the differences between EPI and SNPE in Discussion:

\begin{displayquote}
Lines 435-447\\
``\quoting{discussion_gradients}''
\end{displayquote}

\textbf{4) The discussion should be expanded to be more specific about what problems the authors think the model is, or is not, appropriate for. Comparisons to the Goncalves article would also be helpful since users will want to know the comparative advantages/disadvantages of each method. (if the authors could coordinate running their methods on a common illustrative example, that would be cool, but not required). }

Thank you for this recommendation.
We now include substantial text in discussion devoted to this topic in addition to that quoted in the previous response (R3 concern \#3).

\begin{displayquote}
Lines 427-447\\
``\quoting{discussion_methods} \\ \quoting{discussion_gradients}''
\end{displayquote}

\textbf{5) Given that the paper is heavily a (very valuable!) methods paper for a general audience, the method should be better explained both in the main text and the supplement. Some specific ones are below, but the authors should more generally send the paper to naïve readers to check what is/is not well explained. }

Thank you for this detailed list of improvements to be made when describing the method.

\begin{itemize}
\item \textbf{Figure 1 is somewhat opaque and also has notational issues (e.g. omega is the frequency but also appears to be the random input sample).}

Figure 1 has been completely revised.

\item \textbf{For the general audience of eLife, panels C and D are not well described individually or well connected to each other and don't illustrate or describe all of the relevant variables (including what q0 is and what x is).}

Agreed.  In the new figure, we clearly depict $\mathbf{z}$ as parameters and $\mathbf{x}$ as circuit activity.  
We keep the vertical directionality of panel D exclusively for the deep generative process of the deep probability distribution (where $q_0$ is replaced and shown explicitly as an isotropic gaussian input to the deep network).
The horizontal directionality shared between panels D and E reflect the procedure of theoretical neuroscience described in Section \ref{results_motivating}.

\item \textbf{In equation 2 (and also in the same equation in the supplement), it was not immediately obvious what the expectation was taken over.}

We take care to always indicate the variables over which expectations are taken in the updated manuscript.

\item \textbf{The authors don't specific the distribution of w (it's referred to only as 'a simple random variable', which is not clear).} 

It is clarified in the text that this simple initial distribution transformed by the deep neural network is an isotropic gaussian.

\item \textbf{It was also sometimes hard to quickly find in the text basic, important quantities like what z was for a given simulation.} 

We have made sure to make this explicit in each section of the paper.

\item \textbf{The augmented Lagrangian optimization was not well explained or motivated. There is a reference to m=absolute value(mu) but I didn't see m in the above equation.}

Lines 767-769 \\
\begin{displayquote}
``\quoting{AL_standard}"
\end{displayquote}
 
\item \textbf{Using mu to describe a vector that includes means and variances is confusing notation since mu often denotes means }

Agreed. Throughout the updated main text, we only describe emergent properties through explicit mean and variance constraints as in Equation 11.
We reserve the mean parameterization $\bm{\mu}_{\text{opt}}$ of the maximum entropy solution of the EPI optimization for technical details in methods.
The difference between $\bm{\mu}$ and $\bm{\mu}_{\text{opt}}$ is described in Section \ref{methods_ME_EF}.

\item \textbf{It would be helpful to have a pseudo-code 'Algorithm' figure or section of the text}

We provide pseudocode for the EPI optimization in Algorithm \ref{alg:epi}, which mirrors the pseudocode found in the paper describing the underlying algorithm for MEFN \cite{loaiza2017maximum}.

\end{itemize}


\textbf{Minor Comments: }

\textbf{1) I'm not sure if the authors are referring to a particular constrained form of the Schur decomposition, but the general statement in the Figure caption that the Schur decomposition is unique is not true. Also, one does not need to refer to "Schur eigenvalues" since the diagonal elements of the Schur decomposition are the (usual) eigenvalues. }

We do not use a Schur decomposition to analyze the SC model in the current manuscript.

\textbf{2) p. 31: usually one reserves the variable omega for angular frequencies: omega = 2*pi*f where f is frequency. }

We have removed the variable $\omega$ as it was unnecessary, and avoided introducing another variable $f$ to be conflated with $f$'s use as the emergent property statistics.

\textbf{3) Some references for other approaches and work that might be worth listing for scholarship: Sloppy models and information geometry (including MCMC approaches, e.g. Mannakkee, Ragsdale, Transtrum, Gutenkunst); higher dimensional sloppy models in neuroscience (O'Leary, Sutton, \& Marder 2015, Fisher, Olasagasti, et al., 2013); Compensatory parameter combinations through the implicit function theorem (Olypher and Calabrese, J. Neurophys. 2007). }

Thank you for these references. We have incorporated most of them where appropriate.

\textbf{Additional data files and statistical comments:}

\textbf{Code should be made available in well-documented form if it isn't already. }

Please see the github repository for this manuscript, which includes code for the EPI method and the analyses that went into each figure.
\begin{verbatim} https://github.com/cunningham-lab/epi \end{verbatim}

\end{document}