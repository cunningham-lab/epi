\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{1}}
\citation{kopell1988coupled,marder1998biophysics,abbott2008theoretical,wang2010neurophysiological,o2015computational}
\citation{gutenkunst2007universally,erguler2011practical,mannakee2016sloppiness}
\citation{hopfield1982neural}
\citation{sompolinsky1988chaos}
\citation{olypher2007using}
\citation{tsodyks1997paradoxical}
\citation{wong2006recurrent}
\citation{foster1993significance,prinz2004similar,achard2006complex,fisher2013modeling,o2014cell,alonso2019visualization}
\citation{paninski2018neural}
\citation{niell2010modulation,saleem2013integration,musall2019single}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{2}}
\citation{hopfield1982neural,sompolinsky1988chaos,olypher2007using,tsodyks1997paradoxical,wong2006recurrent}
\citation{beaumont2002approximate,marjoram2003markov,sisson2007sequential}
\citation{raue2009structural,karlsson2012efficient,hines2014determination,raman2017delineating}
\citation{elsayed2017structure,savin2017maximum,mlynarski2020statistical}
\citation{tran2017hierarchical,gonccalves2019training}
\citation{rezende2015variational,papamakarios2019normalizing}
\citation{loaiza2017maximum}
\citation{dinh2017density,kingma2018glow}
\citation{gutierrez2013multiple}
\citation{goldman2001global}
\citation{murphy2009balanced,hennequin2014optimal,bondanelli2019population}
\citation{litwin2016inhibitory,palmigiano2020structure}
\citation{duan2019collicular}
\citation{marder2002cellular}
\citation{goldman2001global,prinz2004similar}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivating emergent property inference of theoretical models}{4}}
\newlabel{results_motivating}{{3.1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Emergent property inference in the stomatogastric ganglion. \textbf  {A}. Conductance-based subcircuit model of the STG. \textbf  {B}. Spiking frequency $\omega (\mathbf  {x}; \mathbf  {z})$ is an emergent property statistic. Simulated at $g_{\text  {el}} = 4.5$nS and $g_{\text  {synA}} = 3$nS. \textbf  {C}. The emergent property of intermediate hub frequency. Simulated activity traces are colored by log probability of generating parameters in the EPI distribution (Panel E). \textbf  {D}. For a choice of circuit model and emergent property, EPI learns a deep probability distribution of parameters $\mathbf  {z}$. \textbf  {E}. The EPI distribution producing intermediate hub frequency. Samples are colored by log probability density. Contours of hub neuron frequency error are shown at levels of $.525$, $.53$, ... $.575$ Hz (dark to light gray away from mean). Dimension of sensitivity $\mathbf  {v}_1$ (solid arrow) and robustness $\mathbf  {v}_2$ (dashed arrow). \textbf  {F} (Top) The predictions of the EPI distribution. The black and gray dashed lines show the mean and two standard deviations according the emergent property. (Bottom) Simulations at the starred parameter values. }}{5}}
\newlabel{fig:STG}{{1}{5}}
\citation{rezende2015variational,papamakarios2019normalizing}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Emergent property inference via deep generative models}{6}}
\newlabel{results_dgm}{{3.2}{6}}
\newlabel{eq:STG_model1}{{1}{6}}
\newlabel{eq:EP_STG1}{{2}{6}}
\citation{rezende2015variational,papamakarios2019normalizing}
\citation{raue2009structural,raman2017delineating}
\newlabel{eq:EP_STG2}{{3}{7}}
\citation{litwin2016inhibitory,palmigiano2020structure}
\citation{duan2019collicular}
\citation{murphy2009balanced,hennequin2014optimal,bondanelli2019population}
\citation{goldman2009memory,murphy2009balanced}
\citation{bondanelli2020coding}
\citation{sussillo2014neural,barak2017recurrent}
\citation{goldman2009memory,murphy2009balanced,bondanelli2020coding}
\citation{sompolinsky1988chaos}
\citation{russo2018motor}
\citation{bondanelli2020coding}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Scaling inference of recurrent neural network connectivity with EPI}{8}}
\newlabel{results_RNN}{{3.3}{8}}
\citation{sisson2007sequential}
\citation{gonccalves2019training}
\citation{sisson2007sequential,gonccalves2019training}
\citation{sisson2018handbook}
\newlabel{eq:EP_LRRNN}{{5}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {A}. Wall time of EPI (blue), SNPE (orange), and SMC-ABC (green) to converge on RNN connectivities producing stable amplification. Each dot shows convergence time for an individual random seed. For reference, the mean wall time for EPI to achieve its full constraint convergence (means and variances) is shown (blue line). \textbf  {B}. Simulation count of each algorithm to achieve convergence. Same conventions as A. \textbf  {C}. The predictive distributions of connectivities inferred by EPI (blue), SNPE (orange), and SMC-ABC (green), with reference to $\mathbf  {x}_0 = \bm  {\mu }$ (gray star). \textbf  {D}. Simulations of networks inferred by each method ($\tau =100ms$). Each trace (15 per algorithm) corresponds to simulation of one $z$. (Below) Ratio of obtained samples producing stable amplification, stable monotonic decay, and instability. }}{10}}
\newlabel{fig:RNN}{{2}{10}}
\citation{cranmer2020frontier}
\citation{tsodyks1997paradoxical}
\citation{goldman2009memory,murphy2009balanced}
\citation{ozeki2009inhibitory}
\citation{rubin2015stabilized}
\citation{hennequin2018dynamical}
\citation{Churchland2010}
\citation{markram2004interneurons,rudy2011three,tremblay2016}
\citation{pfeffer2013inhibition}
\citation{felleman1991distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}EPI reveals how recurrence with multiple inhibitory subtypes governs excitatory variability in a V1 model}{11}}
\newlabel{results_V1}{{3.4}{11}}
\citation{hennequin2018dynamical}
\citation{litwin2016inhibitory}
\citation{palmigiano2020structure}
\newlabel{eq:EP_V1}{{7}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Emergent property inference in the stochastic stabilized supralinear network (SSSN) \textbf  {A}. Four-population model of primary visual cortex with excitatory (black), parvalbumin (blue), somatostatin (red), and VIP (green) neurons (excitatory and inhibitory projections filled and unfilled, respectively). Some neuron-types largely do not form synaptic projections to others ($|W_{\alpha _1, \alpha _2})| < 0.025$). Each neural population receives a baseline input $\mathbf  {h}_b$, and the E- and P-populations also receive a contrast-dependent input $\mathbf  {h}_c$. Additionally, each neural population receives a slow noisy input $\bm  {\epsilon }$. \textbf  {B}. Transient network responses of the SSSN model. Traces are independent trials with varying initialization $\mathbf  {x}(0)$ and noise $\bm  {\epsilon }$. \textbf  {C}. Mean (solid line) and standard deviation $s_E(\mathbf  {x}; \mathbf  {z})$ (shading) across 100 trials. \textbf  {D}. EPI distribution of noise parameters $\mathbf  {z}$ conditioned on E-population variability. The EPI predictive distribution of $s_E(\mathbf  {x}; \mathbf  {z})$ is show on the bottom-left. \textbf  {E}. (Top) Enlarged visualization of the $\sigma _E$-$\sigma _P$ marginal distribution of EPI $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(5\text  {Hz}))$ and $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(10\text  {Hz}))$. Each black dot shows the mode at each $\sigma _P$. The arrows show the most sensitive dimensions of the Hessian evaluated at these modes. \textbf  {F}. The predictive distributions of $\sigma _E^2 + \sigma _P^2$ of each inferred distribution $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(5\text  {Hz}))$ and $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(10\text  {Hz}))$. }}{13}}
\newlabel{fig:V1}{{3}{13}}
\citation{hennequin2018dynamical,Gardiner2009}
\citation{duan2015requirement}
\citation{duan2019collicular}
\citation{duan2019collicular}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}EPI identifies two regimes of rapid task switching}{14}}
\newlabel{results_SC}{{3.5}{14}}
\newlabel{eq:SC_EP}{{9}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \textbf  {A}. Rapid task switching behavioral paradigm (see text). \textbf  {B}. Model of superior colliculus (SC). Neurons: LP - Left Pro, RP - Right Pro, LA - Left Anti, RA - Right Anti. Parameters: $sW$ - self, $hW$ - horizontal, $vW$ -vertical, $dW$ - diagonal weights. \textbf  {C}. The EPI inferred distribution of rapid task switching networks. Red/purple parameters indicate modes $\mathbf  {z}^*(sW)$ colored by $sW$. Sensitivity vectors $\mathbf  {v}_1(\mathbf  {z}^*)$ are shown by arrows. (Bottom-left) EPI predictive distribution of task accuracies. \textbf  {D}. Mean and standard error ($N_{\text  {test}}$ = 25, bars not visible) of accuracy in Pro (top) and Anti (bottom) tasks after perturbing connectivity away from mode along $\mathbf  {v}_1(\mathbf  {z}^*)$ (left), $\mathbf  {v}_{\text  {task}}$ (middle), and $\mathbf  {v}_{\text  {diag}}$ (right). }}{16}}
\newlabel{fig:SC}{{4}{16}}
\citation{duan2019collicular}
\citation{duan2019collicular}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}EPI inferred SC connectivities reproduce results from optogenetic inactivation experiments}{18}}
\newlabel{results_SC_opt}{{3.6}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \textbf  {A}. Mean and standard error (bars) across recording sessions of task error following delay period optogenetic inactivation in rats. \textbf  {B}. Mean and standard deviation (bars) of task error induced by delay period inactivation of varying optogenetic strength $\gamma $ across the EPI distribution. \textbf  {C}. (Left) Mean and standard error of Pro and Anti error from regime 1 to regime 2 at $\gamma = 0.675$. (Right) Correlations of connectivity eigenvalues with Anti error from regime 1 to regime 2 at $\gamma = 0.675$. \textbf  {D}. (Left) Mean and standard deviation (shading) of responses of the SC model at the mode of the EPI distribution to delay period inactivation at $\gamma = 0.85$. Accuracy in Pro (top) and Anti (bottom) task is shown as a percentage. (Right) Anti accuracy following delay period inactivation at $\gamma = 0.85$ versus accuracy in the Pro task across connectivities in the EPI distribution. }}{19}}
\newlabel{fig:SC_opto}{{5}{19}}
\citation{paninski2018neural}
\citation{beaumont2002approximate,marjoram2003markov,sisson2007sequential}
\citation{gonccalves2019training}
\citation{loaiza2017maximum}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{20}}
\newlabel{discussion}{{4}{20}}
\citation{marder1992dynamic}
\citation{prinz2004similar}
\citation{gonccalves2019training}
\citation{girolami2011riemann}
\citation{raue2009structural}
\bibdata{eLife2021}
\bibcite{kopell1988coupled}{1}
\bibcite{marder1998biophysics}{2}
\bibcite{abbott2008theoretical}{3}
\bibcite{wang2010neurophysiological}{4}
\bibcite{o2015computational}{5}
\bibcite{gutenkunst2007universally}{6}
\bibcite{erguler2011practical}{7}
\bibcite{mannakee2016sloppiness}{8}
\bibcite{hopfield1982neural}{9}
\bibcite{sompolinsky1988chaos}{10}
\bibcite{olypher2007using}{11}
\bibcite{tsodyks1997paradoxical}{12}
\bibcite{wong2006recurrent}{13}
\bibcite{foster1993significance}{14}
\bibcite{prinz2004similar}{15}
\bibcite{achard2006complex}{16}
\bibcite{fisher2013modeling}{17}
\bibcite{o2014cell}{18}
\bibcite{alonso2019visualization}{19}
\bibcite{paninski2018neural}{20}
\bibcite{niell2010modulation}{21}
\bibcite{saleem2013integration}{22}
\bibcite{musall2019single}{23}
\bibcite{beaumont2002approximate}{24}
\bibcite{marjoram2003markov}{25}
\bibcite{sisson2007sequential}{26}
\bibcite{raue2009structural}{27}
\bibcite{karlsson2012efficient}{28}
\bibcite{hines2014determination}{29}
\bibcite{raman2017delineating}{30}
\bibcite{elsayed2017structure}{31}
\bibcite{savin2017maximum}{32}
\bibcite{mlynarski2020statistical}{33}
\bibcite{tran2017hierarchical}{34}
\bibcite{gonccalves2019training}{35}
\bibcite{rezende2015variational}{36}
\bibcite{papamakarios2019normalizing}{37}
\bibcite{loaiza2017maximum}{38}
\bibcite{dinh2017density}{39}
\bibcite{kingma2018glow}{40}
\bibcite{gutierrez2013multiple}{41}
\bibcite{goldman2001global}{42}
\bibcite{murphy2009balanced}{43}
\bibcite{hennequin2014optimal}{44}
\bibcite{bondanelli2019population}{45}
\bibcite{litwin2016inhibitory}{46}
\bibcite{palmigiano2020structure}{47}
\bibcite{duan2019collicular}{48}
\bibcite{marder2002cellular}{49}
\bibcite{goldman2009memory}{50}
\bibcite{bondanelli2020coding}{51}
\bibcite{sussillo2014neural}{52}
\bibcite{barak2017recurrent}{53}
\bibcite{russo2018motor}{54}
\bibcite{sisson2018handbook}{55}
\bibcite{cranmer2020frontier}{56}
\bibcite{ozeki2009inhibitory}{57}
\bibcite{rubin2015stabilized}{58}
\bibcite{hennequin2018dynamical}{59}
\bibcite{Churchland2010}{60}
\bibcite{markram2004interneurons}{61}
\bibcite{rudy2011three}{62}
\bibcite{tremblay2016}{63}
\bibcite{pfeffer2013inhibition}{64}
\bibcite{felleman1991distributed}{65}
\bibcite{Gardiner2009}{66}
\bibcite{duan2015requirement}{67}
\bibcite{marder1992dynamic}{68}
\bibcite{girolami2011riemann}{69}
\bibcite{bertsekas2014constrained}{70}
\bibcite{saul1998mean}{71}
\bibcite{metropolis1953equation}{72}
\bibcite{hastings1970monte}{73}
\bibcite{calderhead2011statistical}{74}
\bibcite{golightly2011bayesian}{75}
\bibcite{chkrebtii2016bayesian}{76}
\bibcite{liepe2014framework}{77}
\bibcite{bittner2019degenerate}{78}
\bibcite{bittner2019examining}{79}
\bibcite{nonnenmacher2018sbi}{80}
\bibcite{desitler2019statistical}{81}
\bibcite{LueckmannGoncalves_17}{82}
\bibcite{papamakarios2019sequential}{83}
\bibcite{hermans2020likelihood}{84}
\bibcite{wainwright2008graphical}{85}
\bibcite{bittner2019approximating}{86}
\bibcite{chen2018neural}{87}
\bibcite{li2020scalable}{88}
\bibcite{saccomani2003parameter}{89}
\bibcite{hengl2007data}{90}
\bibcite{papamakarios2017masked}{91}
\bibcite{kingma2016improved}{92}
\bibcite{kingma2014adam}{93}
\bibcite{klinger2018pyabc}{94}
\bibcite{greenberg2019automatic}{95}
\bibcite{Mossing2021}{96}
\bibstyle{unsrt}
\citation{rezende2015variational,papamakarios2019normalizing}
\@writefile{toc}{\contentsline {section}{\numberline {5}Methods}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Emergent property inference (EPI)}{32}}
\newlabel{methods_EPI}{{5.1}{32}}
\newlabel{eq:methods_ep_general}{{12}{32}}
\citation{bertsekas2014constrained}
\citation{loaiza2017maximum}
\newlabel{eq:deep_transform}{{13}{33}}
\newlabel{eq:opt}{{14}{33}}
\citation{saul1998mean}
\citation{metropolis1953equation,hastings1970monte}
\citation{girolami2011riemann}
\citation{calderhead2011statistical}
\citation{golightly2011bayesian}
\citation{chkrebtii2016bayesian}
\citation{cranmer2020frontier}
\citation{beaumont2002approximate}
\citation{marjoram2003markov}
\citation{sisson2007sequential}
\citation{sisson2018handbook}
\citation{liepe2014framework}
\citation{saul1998mean}
\citation{loaiza2017maximum}
\citation{tran2017hierarchical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Related approaches}{34}}
\newlabel{methods_related}{{5.1.1}{34}}
\citation{bittner2019degenerate,bittner2019examining}
\citation{nonnenmacher2018sbi,desitler2019statistical,gonccalves2019training}
\citation{loaiza2017maximum}
\citation{LueckmannGoncalves_17}
\citation{papamakarios2019sequential}
\citation{hermans2020likelihood}
\citation{wainwright2008graphical}
\citation{bittner2019approximating}
\citation{karlsson2012efficient}
\citation{chen2018neural}
\citation{li2020scalable}
\citation{raue2009structural}
\citation{raman2017delineating}
\citation{saccomani2003parameter}
\citation{hengl2007data}
\citation{rezende2015variational,papamakarios2019normalizing}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Deep probability distributions and normalizing flows}{36}}
\newlabel{methods_NF}{{5.1.2}{36}}
\citation{dinh2017density}
\citation{kingma2018glow}
\citation{papamakarios2017masked,kingma2016improved}
\citation{kingma2016improved,papamakarios2017masked}
\citation{wainwright2008graphical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Maximum entropy distributions and exponential families}{37}}
\newlabel{methods_ME_EF}{{5.1.3}{37}}
\newlabel{eq:max_ent}{{16}{37}}
\citation{kingma2014adam}
\newlabel{eq:moments}{{19}{38}}
\newlabel{eq:mu_opt}{{20}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Augmented lagrangian optimization}{38}}
\newlabel{methods_AL_opt}{{5.1.4}{38}}
\newlabel{eq:AL}{{21}{38}}
\citation{kingma2014adam}
\citation{bittner2019approximating}
\newlabel{alg:epi}{{1}{40}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Emergent property inference}}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.5}Example: 2D LDS}{41}}
\newlabel{methods_2DLDS}{{5.1.5}{41}}
\citation{wainwright2008graphical}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces \textbf  {A}. Two-dimensional linear dynamical system model, where real entries of the dynamics matrix $A$ are the parameters. \textbf  {B}. The EPI distribution for a two-dimensional linear dynamical system with $\tau =1$ that produces an average of 1Hz oscillations with some small amount of variance. Dashed lines indicate the parameter axes. \textbf  {C}. Entropy throughout the optimization. At the beginning of each augmented lagrangian epoch ($i_{\text  {max}} = 2,000$ iterations), the entropy dipped due to the shifted optimization manifold where emergent property constraint satisfaction is increasingly weighted. \textbf  {D}. Emergent property moments throughout optimization. At the beginning of each augmented lagrangian epoch, the emergent property moments adjust closer to their constraints.}}{43}}
\newlabel{fig:LDS1}{{S1}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.6}EPI as variational inference}{44}}
\newlabel{methods_VI}{{5.1.6}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces \textbf  {A}. Probability contours in the $a_{1,1}$-$a_{2,2}$ plane were derived from the relationship to emergent property statistic of growth/decay factor $\text  {real}(\lambda _1)$. \textbf  {B}. Probability contours in the $a_{1,2}$-$a_{2,1}$ plane were derived from the emergent property statistic of oscillation frequency $2\pi \text  {imag}(\lambda _1)$.}}{45}}
\newlabel{fig:LDS2}{{S2}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {S3}{\ignorespaces Sampled dynamical systems $\mathbf  {z} \sim q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X})$ and their simulated activity from $\mathbf  {x}(t=0) = [\frac  {\sqrt  {2}}{2}, -\frac  {\sqrt  {2}}{2}]$ colored by log probability. \textbf  {A}. Each dimension of the simulated trajectories throughout time. \textbf  {B}. The simulated trajectories in phase space.}}{45}}
\newlabel{fig:LDS3}{{S3}{45}}
\citation{gutierrez2013multiple}
\citation{gutierrez2013multiple}
\citation{marder1992dynamic}
\citation{prinz2004similar,gonccalves2019training}
\newlabel{eq:VI_opt}{{36}{46}}
\newlabel{eq:epi_vi}{{40}{46}}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Stomatogastric ganglion}{47}}
\newlabel{methods_STG}{{5.2}{47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}STG model}{47}}
\newlabel{methods_stg_model}{{5.2.1}{47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Hub frequency calculation}{49}}
\newlabel{methods_stg_hub}{{5.2.2}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}EPI details for the STG model}{49}}
\newlabel{methods_stg_epi}{{5.2.3}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {S4}{\ignorespaces EPI optimization of the STG model producing network syncing. \textbf  {A}. Entropy throughout optimization. \textbf  {B}. The emergent property statistic means and variances converge to their constraints at 25,000 iterations following the fifth augmented lagrangian epoch.}}{50}}
\newlabel{fig:STG1}{{S4}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Hessian sensitivity vectors}{50}}
\newlabel{methods_stg_hessian}{{5.2.4}{50}}
\citation{sisson2007sequential}
\citation{gonccalves2019training}
\citation{bondanelli2020coding}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Scaling EPI for stable amplification in RNNs}{51}}
\newlabel{methods_RNN}{{5.3}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Rank-2 RNN model}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Stable amplification}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}EPI details for RNNs}{51}}
\citation{beaumont2002approximate}
\citation{klinger2018pyabc}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Methodological comparison}{52}}
\newlabel{eq:stab_amp_x0}{{67}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {S5}{\ignorespaces Number of parameters in deep probability distribution architectures of EPI (blue) and SNPE (orange) by RNN size ($N$). }}{53}}
\newlabel{fig:RNN1}{{S5}{53}}
\citation{greenberg2019automatic}
\citation{greenberg2019automatic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.5}Effect of RNN parameters on EPI and SNPE inferred distributions}{54}}
\newlabel{methods_RNN_params}{{5.3.5}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {S6}{\ignorespaces SNPE convergence was enabled by increasing $n_{\text  {round}}$, not $n_{\text  {atom}}$. \textbf  {A}. Difference of mean predictions $\mathbf  {x}_0$ throughout optimization at $N=50$ with by simulation count (left) and wall time (right) of SNPE with $n_{\text  {round}} = 5,000$ (light orange), SNPE with $n_{\text  {round}} = 25,000$ (dark orange), and EPI (blue). Each line shows an individual random seed. \textbf  {B}. Same conventions as A at $N=100$ of SNPE with $n_{\text  {atom}} = 100$ (light orange) and $n_{\text  {atom}} = 1,000$ (dark orange). \textbf  {C}. Same conventions as A at $N=100$ of SNPE with $n_{\text  {round}} = 25,000$ (light orange) and $n_{\text  {round}} = 250,000$ (dark orange). }}{55}}
\newlabel{fig:RNN3}{{S6}{55}}
\citation{tremblay2016}
\citation{markram2004interneurons,rudy2011three,tremblay2016}
\citation{pfeffer2013inhibition}
\citation{Mossing2021,palmigiano2020structure}
\citation{hennequin2018dynamical}
\citation{palmigiano2020structure}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Primary visual cortex}{56}}
\newlabel{methods_V1}{{5.4}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}V1 model}{56}}
\newlabel{sigma_reparam}{{70}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {S7}{\ignorespaces Model characteristics affect predictions of posteriors inferred by SNPE, while predictions of parameters inferred by EPI remain fixed. \textbf  {A}. Predictive distribution of EPI (blue) and SNPE (orange) inferred connectivity of RNNs exhibiting stable amplification with $N=2$ (top), $N=10$ (bottom), $g=0.01$ (left), and $g=0.1$ (right). \textbf  {B}. Entropy of parameter distribution approximations throughout optimization with $N=2$ (top), $N=10$ (bottom), $g=0.1$ (dark shade), and $g=0.01$ (light shade). \textbf  {C}. Validation log probabilities throughout SNPE optimization. Same conventions as B. \textbf  {D}. Adherence to EPI constraints. Same conventions as B. }}{57}}
\newlabel{fig:RNN2}{{S7}{57}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}EPI details for the V1 model}{59}}
\citation{tsodyks1997paradoxical}
\@writefile{lof}{\contentsline {figure}{\numberline {S8}{\ignorespaces EPI inferred distribution for $\mathcal  {X}(10\text  {Hz})$. }}{60}}
\newlabel{fig:V1_1}{{S8}{60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Sensitivity analyses}{60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.4}Testing for the paradoxical effect}{60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.5}Primary visual cortex: Mathematical intuition and challenges}{60}}
\newlabel{methods_V1_complexity}{{5.4.5}{60}}
\citation{hennequin2018dynamical,Gardiner2009}
\@writefile{lof}{\contentsline {figure}{\numberline {S9}{\ignorespaces EPI optimization $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(5\text  {Hz}))$ \textbf  {A}. Entropy throughout optimization. \textbf  {B}. The emergent property statistic means and variances converge to their constraints at 8,000 iterations following the fourth augmented lagrangian epoch. }}{61}}
\newlabel{fig:V1_2}{{S9}{61}}
\newlabel{SSSN_agos}{{81}{61}}
\citation{Gardiner2009}
\@writefile{lof}{\contentsline {figure}{\numberline {S10}{\ignorespaces EPI predictive distributions of the sum of squares of each pair of noise parameters. }}{62}}
\newlabel{fig:V1_3}{{S10}{62}}
\newlabel{GeneralEllipsoid}{{86}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {S11}{\ignorespaces (Left) SSSN simulations for small increases in neuron-type population input. (Right) Average (solid) and standard deviation (shaded) of stochastic fluctuations of responses. }}{63}}
\newlabel{fig:V1_4}{{S11}{63}}
\citation{duan2015requirement}
\citation{duan2019collicular}
\citation{duan2019collicular}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Superior colliculus}{64}}
\newlabel{methods_SC}{{5.5}{64}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}SC model}{64}}
\newlabel{methods_sc_model}{{5.5.1}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {S12}{\ignorespaces \textbf  {A}. Same pairplot as Fig. 4\hbox {}C colored by Pro task accuracy. \textbf  {B}. Same as A colored by Anti task accuracy. \textbf  {C}. Connectivity parameters of EPI distributions versus task accuracies. $\beta $ is slope coefficient of linear regression, $r$ is correlation, and $p$ is the two-tailed p-value. }}{65}}
\newlabel{fig:SC1}{{S12}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {S13}{\ignorespaces  \textbf  {A}. Simulations in network regime 1: $\mathbf  {z}^*(sW=-0.75)$. \textbf  {B}. Simulations in network regime 2: $\mathbf  {z}^*(sW=0.75)$. }}{66}}
\newlabel{fig:SC2}{{S13}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {S14}{\ignorespaces  \textbf  {A}. Invariant eigenvectors of connectivity matrix $W$. \textbf  {B}. Accuracies for connectivity perturbations when changing $\lambda _{\text  {all}}$ and $\lambda _{\text  {side}}$ ($\lambda _{\text  {task}}$ and $\lambda _{\text  {diag}}$ shown in Fig. 4\hbox {}D). }}{67}}
\newlabel{fig:SC3}{{S14}{67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Task accuracy calculation}{67}}
\newlabel{methods_sc_acc}{{5.5.2}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {S15}{\ignorespaces  (Left) Mean and standard error of Pro and Anti error from regime 1 to regime 2 at $\gamma = 0.85$. (Right) Correlations of connectivity eigenvalues with Anti error from regime 1 to regime 2 at $\gamma = 0.85$. }}{68}}
\newlabel{fig:SC5}{{S15}{68}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}EPI details for the SC model}{68}}
\newlabel{methods_sc_epi}{{5.5.3}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {S16}{\ignorespaces  (Left) Mean and standard deviation (shading) of responses of the SC model at the mode of the EPI distribution to delay period inactivation at $\gamma = 0.675$. Accuracy in Pro (top) and Anti (bottom) task is shown as a percentage. (Right) Anti accuracy following delay period inactivation at $\gamma = 0.675$ versus accuracy in the Pro task across connectivities in the EPI distribution. }}{69}}
\newlabel{fig:SC6}{{S16}{69}}
\@writefile{lof}{\contentsline {figure}{\numberline {S17}{\ignorespaces EPI optimization of the SC model producing rapid task switching. \textbf  {A}. Entropy throughout optimization. \textbf  {B}. The emergent property statistic means and variances converge to their constraints at 20,000 iterations following the tenth augmented lagrangian epoch. }}{69}}
\newlabel{fig:SC7}{{S17}{69}}
\@writefile{lof}{\contentsline {figure}{\numberline {S18}{\ignorespaces  \textbf  {A}.Rapid task switching SC connectivities obtained from random sampling. \textbf  {B}. Task accuracies of the inferred distributions from random sampling (top) and EPI (bottom). }}{70}}
\newlabel{fig:SC8}{{S18}{70}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}Mode identification with EPI}{70}}
\newlabel{methods_sc_regime}{{5.5.4}{70}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5}Sample grouping by mode}{71}}
\newlabel{methods_sc_group}{{5.5.5}{71}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.6}Sensitivity analysis}{71}}
\newlabel{methods_sc_hessian}{{5.5.6}{71}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.7}Connectivity eigendecomposition and processing modes}{71}}
\newlabel{methods_sc_eig}{{5.5.7}{71}}
\citation{duan2019collicular}
\newlabel{eq:dzdlambda}{{109}{72}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.8}Modeling optogenetic silencing.}{72}}
\newlabel{methods_sc_opto}{{5.5.8}{72}}
