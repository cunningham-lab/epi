\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{1}}
\citation{kopell1988coupled,marder1998biophysics,abbott2008theoretical,wang2010neurophysiological,o2015computational}
\citation{gutenkunst2007universally,erguler2011practical,mannakee2016sloppiness}
\citation{hopfield1982neural}
\citation{sompolinsky1988chaos}
\citation{olypher2007using}
\citation{tsodyks1997paradoxical}
\citation{wong2006recurrent}
\citation{foster1993significance,prinz2004similar,achard2006complex,fisher2013modeling,o2014cell,alonso2019visualization}
\citation{paninski2018neural}
\citation{niell2010modulation,saleem2013integration,musall2019single}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{2}}
\citation{hopfield1982neural,sompolinsky1988chaos,olypher2007using,tsodyks1997paradoxical,wong2006recurrent}
\citation{beaumont2002approximate,marjoram2003markov,sisson2007sequential}
\citation{raue2009structural,karlsson2012efficient,hines2014determination,raman2017delineating}
\citation{elsayed2017structure,savin2017maximum,mlynarski2020statistical}
\citation{tran2017hierarchical,gonccalves2019training}
\citation{rezende2015variational,papamakarios2019normalizing}
\citation{loaiza2017maximum}
\citation{dinh2017density,kingma2018glow}
\citation{gutierrez2013multiple}
\citation{goldman2001global}
\citation{murphy2009balanced,hennequin2014optimal,bondanelli2019population}
\citation{litwin2016inhibitory,palmigiano2020structure}
\citation{duan2021collicular}
\citation{marder2002cellular}
\citation{goldman2001global,prinz2004similar}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivating emergent property inference of theoretical models}{4}}
\newlabel{results_motivating}{{3.1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Emergent property inference in the stomatogastric ganglion. \textbf  {A}. Conductance-based subcircuit model of the STG. \textbf  {B}. Spiking frequency $\omega (\mathbf  {x}; \mathbf  {z})$ is an emergent property statistic. Simulated at $g_{\text  {el}} = 4.5$nS and $g_{\text  {synA}} = 3$nS. \textbf  {C}. The emergent property of intermediate hub frequency. Simulated activity traces are colored by log probability of generating parameters in the EPI distribution (Panel E). \textbf  {D}. For a choice of circuit model and emergent property, EPI learns a deep probability distribution of parameters $\mathbf  {z}$. \textbf  {E}. The EPI distribution producing intermediate hub frequency. Samples are colored by log probability density. Contours of hub neuron frequency error are shown at levels of $.525$, $.53$, ... $.575$ Hz (dark to light gray away from mean). Dimension of sensitivity $\mathbf  {v}_1$ (solid arrow) and robustness $\mathbf  {v}_2$ (dashed arrow). \textbf  {F} (Top) The predictions of the EPI distribution. The black and gray dashed lines show the mean and two standard deviations according the emergent property. (Bottom) Simulations at the starred parameter values. \relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:STG}{{1}{5}}
\citation{rezende2015variational,papamakarios2019normalizing}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Emergent property inference via deep generative models}{6}}
\newlabel{results_dgm}{{3.2}{6}}
\newlabel{eq:STG_model1}{{1}{6}}
\newlabel{eq:EP_STG1}{{2}{6}}
\citation{rezende2015variational,papamakarios2019normalizing}
\citation{raue2009structural,raman2017delineating}
\newlabel{eq:EP_STG2}{{3}{7}}
\citation{litwin2016inhibitory,palmigiano2020structure}
\citation{duan2021collicular}
\citation{murphy2009balanced,hennequin2014optimal,bondanelli2019population}
\citation{goldman2009memory,murphy2009balanced}
\citation{bondanelli2020coding}
\citation{sussillo2014neural,barak2017recurrent}
\citation{goldman2009memory,murphy2009balanced,bondanelli2020coding}
\citation{sompolinsky1988chaos}
\citation{russo2018motor}
\citation{bondanelli2020coding}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Scaling inference of recurrent neural network connectivity with EPI}{8}}
\newlabel{results_RNN}{{3.3}{8}}
\citation{sisson2007sequential}
\citation{gonccalves2019training}
\citation{sisson2007sequential,gonccalves2019training}
\citation{sisson2018handbook}
\newlabel{eq:EP_LRRNN}{{5}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  \textbf  {A}. Wall time of EPI (blue), SNPE (orange), and SMC-ABC (green) to converge on RNN connectivities producing stable amplification. Each dot shows convergence time for an individual random seed. For reference, the mean wall time for EPI to achieve its full constraint convergence (means and variances) is shown (blue line). \textbf  {B}. Simulation count of each algorithm to achieve convergence. Same conventions as A. \textbf  {C}. The predictive distributions of connectivities inferred by EPI (blue), SNPE (orange), and SMC-ABC (green), with reference to $\mathbf  {x}_0 = \bm  {\mu }$ (gray star). \textbf  {D}. Simulations of networks inferred by each method ($\tau =100ms$). Each trace (15 per algorithm) corresponds to simulation of one $z$. (Below) Ratio of obtained samples producing stable amplification, stable monotonic decay, and instability. \relax }}{10}}
\newlabel{fig:RNN}{{2}{10}}
\citation{cranmer2020frontier}
\citation{tsodyks1997paradoxical}
\citation{goldman2009memory,murphy2009balanced}
\citation{ozeki2009inhibitory}
\citation{rubin2015stabilized}
\citation{hennequin2018dynamical}
\citation{Churchland2010}
\citation{markram2004interneurons,rudy2011three,tremblay2016}
\citation{pfeffer2013inhibition}
\citation{felleman1991distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}EPI reveals how recurrence with multiple inhibitory subtypes governs excitatory variability in a V1 model}{11}}
\newlabel{results_V1}{{3.4}{11}}
\citation{hennequin2018dynamical}
\citation{litwin2016inhibitory}
\citation{palmigiano2020structure}
\newlabel{eq:EP_V1}{{7}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Emergent property inference in the stochastic stabilized supralinear network (SSSN) \textbf  {A}. Four-population model of primary visual cortex with excitatory (black), parvalbumin (blue), somatostatin (red), and VIP (green) neurons (excitatory and inhibitory projections filled and unfilled, respectively). Some neuron-types largely do not form synaptic projections to others ($|W_{\alpha _1, \alpha _2})| < 0.025$). Each neural population receives a baseline input $\mathbf  {h}_b$, and the E- and P-populations also receive a contrast-dependent input $\mathbf  {h}_c$. Additionally, each neural population receives a slow noisy input $\bm  {\epsilon }$. \textbf  {B}. Transient network responses of the SSSN model. Traces are independent trials with varying initialization $\mathbf  {x}(0)$ and noise $\bm  {\epsilon }$. \textbf  {C}. Mean (solid line) and standard deviation $s_E(\mathbf  {x}; \mathbf  {z})$ (shading) across 100 trials. \textbf  {D}. EPI distribution of noise parameters $\mathbf  {z}$ conditioned on E-population variability. The EPI predictive distribution of $s_E(\mathbf  {x}; \mathbf  {z})$ is show on the bottom-left. \textbf  {E}. (Top) Enlarged visualization of the $\sigma _E$-$\sigma _P$ marginal distribution of EPI $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(5\text  {Hz}))$ and $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(10\text  {Hz}))$. Each black dot shows the mode at each $\sigma _P$. The arrows show the most sensitive dimensions of the Hessian evaluated at these modes. \textbf  {F}. The predictive distributions of $\sigma _E^2 + \sigma _P^2$ of each inferred distribution $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(5\text  {Hz}))$ and $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X}(10\text  {Hz}))$. \relax }}{13}}
\newlabel{fig:V1}{{3}{13}}
\citation{hennequin2018dynamical,Gardiner2009}
\citation{duan2015requirement}
\citation{duan2021collicular}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}EPI identifies two regimes of rapid task switching}{14}}
\newlabel{results_SC}{{3.5}{14}}
\citation{duan2021collicular}
\newlabel{eq:SC_EP}{{9}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 4.5\p@ plus2\p@ minus\p@ \topsep 9\p@ plus3\p@ minus5\p@ \itemsep 4.5\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \textbf  {A}. Rapid task switching behavioral paradigm (see text). \textbf  {B}. Model of superior colliculus (SC). Neurons: LP - Left Pro, RP - Right Pro, LA - Left Anti, RA - Right Anti. Parameters: $sW$ - self, $hW$ - horizontal, $vW$ -vertical, $dW$ - diagonal weights. \textbf  {C}. The EPI inferred distribution of rapid task switching networks. Red/purple parameters indicate modes $\mathbf  {z}^*(sW)$ colored by $sW$. Sensitivity vectors $\mathbf  {v}_1(\mathbf  {z}^*)$ are shown by arrows. (Bottom-left) EPI predictive distribution of task accuracies. \textbf  {D}. Mean and standard error ($N_{\text  {test}}$ = 25, bars not visible) of accuracy in Pro (top) and Anti (bottom) tasks after perturbing connectivity away from mode along $\mathbf  {v}_1(\mathbf  {z}^*)$ (left), $\mathbf  {v}_{\text  {task}}$ (middle), and $\mathbf  {v}_{\text  {diag}}$ (right). \relax }}{16}}
\newlabel{fig:SC}{{4}{16}}
\citation{duan2021collicular}
\citation{duan2021collicular}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}EPI inferred SC connectivities reproduce results from optogenetic inactivation experiments}{18}}
\newlabel{results_SC_opt}{{3.6}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \textbf  {A}. Mean and standard error (bars) across recording sessions of task error following delay period optogenetic inactivation in rats. \textbf  {B}. Mean and standard deviation (bars) of task error induced by delay period inactivation of varying optogenetic strength $\gamma $ across the EPI distribution. \textbf  {C}. (Left) Mean and standard error of Pro and Anti error from regime 1 to regime 2 at $\gamma = 0.675$. (Right) Correlations of connectivity eigenvalues with Anti error from regime 1 to regime 2 at $\gamma = 0.675$. \textbf  {D}. (Left) Mean and standard deviation (shading) of responses of the SC model at the mode of the EPI distribution to delay period inactivation at $\gamma = 0.85$. Accuracy in Pro (top) and Anti (bottom) task is shown as a percentage. (Right) Anti accuracy following delay period inactivation at $\gamma = 0.85$ versus accuracy in the Pro task across connectivities in the EPI distribution. \relax }}{19}}
\newlabel{fig:SC_opto}{{5}{19}}
\citation{paninski2018neural}
\citation{beaumont2002approximate,marjoram2003markov,sisson2007sequential}
\citation{gonccalves2019training}
\citation{loaiza2017maximum}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{20}}
\newlabel{discussion}{{4}{20}}
\citation{marder1992dynamic}
\citation{prinz2004similar}
\citation{gonccalves2019training}
\citation{girolami2011riemann}
\citation{raue2009structural}
\bibdata{eLife2021}
\bibcite{kopell1988coupled}{1}
\bibcite{marder1998biophysics}{2}
\bibcite{abbott2008theoretical}{3}
\bibcite{wang2010neurophysiological}{4}
\bibcite{o2015computational}{5}
\bibcite{gutenkunst2007universally}{6}
\bibcite{erguler2011practical}{7}
\bibcite{mannakee2016sloppiness}{8}
\bibcite{hopfield1982neural}{9}
\bibcite{sompolinsky1988chaos}{10}
\bibcite{olypher2007using}{11}
\bibcite{tsodyks1997paradoxical}{12}
\bibcite{wong2006recurrent}{13}
\bibcite{foster1993significance}{14}
\bibcite{prinz2004similar}{15}
\bibcite{achard2006complex}{16}
\bibcite{fisher2013modeling}{17}
\bibcite{o2014cell}{18}
\bibcite{alonso2019visualization}{19}
\bibcite{paninski2018neural}{20}
\bibcite{niell2010modulation}{21}
\bibcite{saleem2013integration}{22}
\bibcite{musall2019single}{23}
\bibcite{beaumont2002approximate}{24}
\bibcite{marjoram2003markov}{25}
\bibcite{sisson2007sequential}{26}
\bibcite{raue2009structural}{27}
\bibcite{karlsson2012efficient}{28}
\bibcite{hines2014determination}{29}
\bibcite{raman2017delineating}{30}
\bibcite{elsayed2017structure}{31}
\bibcite{savin2017maximum}{32}
\bibcite{mlynarski2020statistical}{33}
\bibcite{tran2017hierarchical}{34}
\bibcite{gonccalves2019training}{35}
\bibcite{rezende2015variational}{36}
\bibcite{papamakarios2019normalizing}{37}
\bibcite{loaiza2017maximum}{38}
\bibcite{dinh2017density}{39}
\bibcite{kingma2018glow}{40}
\bibcite{gutierrez2013multiple}{41}
\bibcite{goldman2001global}{42}
\bibcite{murphy2009balanced}{43}
\bibcite{hennequin2014optimal}{44}
\bibcite{bondanelli2019population}{45}
\bibcite{litwin2016inhibitory}{46}
\bibcite{palmigiano2020structure}{47}
\bibcite{duan2021collicular}{48}
\bibcite{marder2002cellular}{49}
\bibcite{goldman2009memory}{50}
\bibcite{bondanelli2020coding}{51}
\bibcite{sussillo2014neural}{52}
\bibcite{barak2017recurrent}{53}
\bibcite{russo2018motor}{54}
\bibcite{sisson2018handbook}{55}
\bibcite{cranmer2020frontier}{56}
\bibcite{ozeki2009inhibitory}{57}
\bibcite{rubin2015stabilized}{58}
\bibcite{hennequin2018dynamical}{59}
\bibcite{Churchland2010}{60}
\bibcite{markram2004interneurons}{61}
\bibcite{rudy2011three}{62}
\bibcite{tremblay2016}{63}
\bibcite{pfeffer2013inhibition}{64}
\bibcite{felleman1991distributed}{65}
\bibcite{Gardiner2009}{66}
\bibcite{duan2015requirement}{67}
\bibcite{marder1992dynamic}{68}
\bibcite{girolami2011riemann}{69}
\bibcite{bertsekas2014constrained}{70}
\bibcite{saul1998mean}{71}
\bibcite{metropolis1953equation}{72}
\bibcite{hastings1970monte}{73}
\bibcite{calderhead2011statistical}{74}
\bibcite{golightly2011bayesian}{75}
\bibcite{chkrebtii2016bayesian}{76}
\bibcite{liepe2014framework}{77}
\bibcite{bittner2019degenerate}{78}
\bibcite{bittner2019examining}{79}
\bibcite{nonnenmacher2018sbi}{80}
\bibcite{desitler2019statistical}{81}
\bibcite{LueckmannGoncalves_17}{82}
\bibcite{papamakarios2019sequential}{83}
\bibcite{hermans2020likelihood}{84}
\bibcite{wainwright2008graphical}{85}
\bibcite{bittner2019approximating}{86}
\bibcite{chen2018neural}{87}
\bibcite{li2020scalable}{88}
\bibcite{saccomani2003parameter}{89}
\bibcite{hengl2007data}{90}
\bibcite{papamakarios2017masked}{91}
\bibcite{kingma2016improved}{92}
\bibcite{kingma2014adam}{93}
\bibcite{klinger2018pyabc}{94}
\bibcite{greenberg2019automatic}{95}
\bibcite{Mossing2021}{96}
\bibstyle{unsrt}
\citation{rezende2015variational,papamakarios2019normalizing}
\@writefile{toc}{\contentsline {section}{\numberline {5}Methods}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Emergent property inference (EPI)}{32}}
\newlabel{methods_EPI}{{5.1}{32}}
\newlabel{eq:methods_ep_general}{{12}{32}}
\citation{bertsekas2014constrained}
\citation{loaiza2017maximum}
\newlabel{eq:deep_transform}{{13}{33}}
\newlabel{eq:opt}{{14}{33}}
\citation{saul1998mean}
\citation{metropolis1953equation,hastings1970monte}
\citation{girolami2011riemann}
\citation{calderhead2011statistical}
\citation{golightly2011bayesian}
\citation{chkrebtii2016bayesian}
\citation{cranmer2020frontier}
\citation{beaumont2002approximate}
\citation{marjoram2003markov}
\citation{sisson2007sequential}
\citation{sisson2018handbook}
\citation{liepe2014framework}
\citation{saul1998mean}
\citation{loaiza2017maximum}
\citation{tran2017hierarchical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Related approaches}{34}}
\newlabel{methods_related}{{5.1.1}{34}}
\citation{bittner2019degenerate,bittner2019examining}
\citation{nonnenmacher2018sbi,desitler2019statistical,gonccalves2019training}
\citation{loaiza2017maximum}
\citation{LueckmannGoncalves_17}
\citation{papamakarios2019sequential}
\citation{hermans2020likelihood}
\citation{wainwright2008graphical}
\citation{bittner2019approximating}
\citation{karlsson2012efficient}
\citation{chen2018neural}
\citation{li2020scalable}
\citation{raue2009structural}
\citation{raman2017delineating}
\citation{saccomani2003parameter}
\citation{hengl2007data}
\citation{rezende2015variational,papamakarios2019normalizing}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Deep probability distributions and normalizing flows}{36}}
\newlabel{methods_NF}{{5.1.2}{36}}
\citation{dinh2017density}
\citation{kingma2018glow}
\citation{papamakarios2017masked,kingma2016improved}
\citation{kingma2016improved,papamakarios2017masked}
\citation{wainwright2008graphical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Maximum entropy distributions and exponential families}{37}}
\newlabel{methods_ME_EF}{{5.1.3}{37}}
\newlabel{eq:max_ent}{{16}{37}}
\citation{kingma2014adam}
\newlabel{eq:moments}{{19}{38}}
\newlabel{eq:mu_opt}{{20}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Augmented lagrangian optimization}{38}}
\newlabel{methods_AL_opt}{{5.1.4}{38}}
\newlabel{eq:AL}{{21}{38}}
\citation{kingma2014adam}
\citation{bittner2019approximating}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Emergent property inference\relax }}{40}}
\newlabel{alg:epi}{{1}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.5}Example: 2D LDS}{41}}
\newlabel{methods_2DLDS}{{5.1.5}{41}}
\citation{wainwright2008graphical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.6}EPI as variational inference}{46}}
\newlabel{methods_VI}{{5.1.6}{46}}
\newlabel{eq:VI_opt}{{36}{46}}
\newlabel{eq:epi_vi}{{40}{46}}
\citation{gutierrez2013multiple}
\citation{gutierrez2013multiple}
\citation{marder1992dynamic}
\citation{prinz2004similar,gonccalves2019training}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Stomatogastric ganglion}{47}}
\newlabel{methods_STG}{{5.2}{47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}STG model}{47}}
\newlabel{methods_stg_model}{{5.2.1}{47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Hub frequency calculation}{49}}
\newlabel{methods_stg_hub}{{5.2.2}{49}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}EPI details for the STG model}{50}}
\newlabel{methods_stg_epi}{{5.2.3}{50}}
\citation{sisson2007sequential}
\citation{gonccalves2019training}
\citation{bondanelli2020coding}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Hessian sensitivity vectors}{51}}
\newlabel{methods_stg_hessian}{{5.2.4}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Scaling EPI for stable amplification in RNNs}{51}}
\newlabel{methods_RNN}{{5.3}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Rank-2 RNN model}{51}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Stable amplification}{51}}
\citation{beaumont2002approximate}
\citation{klinger2018pyabc}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}EPI details for RNNs}{52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Methodological comparison}{52}}
\citation{greenberg2019automatic}
\citation{greenberg2019automatic}
\newlabel{eq:stab_amp_x0}{{67}{53}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.5}Effect of RNN parameters on EPI and SNPE inferred distributions}{54}}
\newlabel{methods_RNN_params}{{5.3.5}{54}}
\citation{tremblay2016}
\citation{markram2004interneurons,rudy2011three,tremblay2016}
\citation{pfeffer2013inhibition}
\citation{Mossing2021,palmigiano2020structure}
\citation{hennequin2018dynamical}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Primary visual cortex}{56}}
\newlabel{methods_V1}{{5.4}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}V1 model}{56}}
\citation{palmigiano2020structure}
\newlabel{sigma_reparam}{{70}{58}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}EPI details for the V1 model}{59}}
\citation{tsodyks1997paradoxical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Sensitivity analyses}{60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.4}Testing for the paradoxical effect}{61}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.5}Primary visual cortex: Mathematical intuition and challenges}{61}}
\newlabel{methods_V1_complexity}{{5.4.5}{61}}
\newlabel{SSSN_agos}{{81}{61}}
\citation{hennequin2018dynamical,Gardiner2009}
\citation{Gardiner2009}
\citation{duan2015requirement}
\citation{duan2021collicular}
\newlabel{GeneralEllipsoid}{{86}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Superior colliculus}{64}}
\newlabel{methods_SC}{{5.5}{64}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}SC model}{64}}
\newlabel{methods_sc_model}{{5.5.1}{64}}
\citation{duan2021collicular}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Task accuracy calculation}{67}}
\newlabel{methods_sc_acc}{{5.5.2}{67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}EPI details for the SC model}{69}}
\newlabel{methods_sc_epi}{{5.5.3}{69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}Mode identification with EPI}{71}}
\newlabel{methods_sc_regime}{{5.5.4}{71}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.5}Sample grouping by mode}{71}}
\newlabel{methods_sc_group}{{5.5.5}{71}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.6}Sensitivity analysis}{72}}
\newlabel{methods_sc_hessian}{{5.5.6}{72}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.7}Connectivity eigendecomposition and processing modes}{72}}
\newlabel{methods_sc_eig}{{5.5.7}{72}}
\citation{duan2021collicular}
\newlabel{eq:dzdlambda}{{109}{73}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.8}Modeling optogenetic silencing.}{73}}
\newlabel{methods_sc_opto}{{5.5.8}{73}}
