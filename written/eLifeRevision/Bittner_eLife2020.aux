\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{1}\protected@file@percent }
\citation{abbott2008theoretical}
\citation{hopfield1982neural}
\citation{sompolinsky1988chaos}
\citation{tsodyks1997paradoxical}
\citation{wong2006recurrent}
\citation{liepe2014framework}
\citation{kingma2013auto}
\citation{rezende2014stochastic}
\citation{gao2016linear}
\citation{zhao2017recursive}
\citation{barello2018sparse}
\citation{pandarinath2018inferring}
\citation{wiltschko2015mapping}
\citation{johnson2016composing}
\citation{batty2019behavenet}
\citation{paninski2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{2}\protected@file@percent }
\citation{raue2009structural}
\citation{gelman2013philosophy}
\citation{blei2014build}
\citation{transtrum2015perspective}
\citation{tran2017hierarchical}
\citation{rezende2015variational}
\citation{dinh2016density}
\citation{papamakarios2017masked}
\citation{loaiza2017maximum}
\citation{goldman2001global}
\citation{gutierrez2013multiple}
\citation{litwin2016inhibitory}
\citation{duan2018collicular}
\citation{mastrogiuseppe2018linking}
\citation{marder2002cellular}
\citation{goldman2001global}
\citation{prinz2004similar}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivating emergent property inference of theoretical models}{4}\protected@file@percent }
\newlabel{results_motivating}{{3.1}{4}}
\citation{gutierrez2013multiple}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}A deep generative modeling approach to emergent property inference}{5}\protected@file@percent }
\newlabel{results_dgm}{{3.2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \small  Emergent property inference (EPI) in the stomatogastric ganglion. A. For a choice of model (STG) and emergent property (unified intermediacy), emergent property inference (EPI) learns a distribution of the model parameters $\mathbf  {z} = \left [g_{\text  {el}}, g_{\text  {synA}} \right ]$ producing unified intermediacy. In the STG model, jagged connections indicate electrical coupling having electrical conductance $g_{\text  {el}}$. Other connections in the diagram are inhibitory synaptic projections having strength $g_{\text  {synA}}$ onto the hub neuron, and $g_{\text  {synB}}=5$nS for mutual inhibitory connections. Deep probability distributions map a simple random variable $\mathbf  {w}$ through a deep neural network with weights and biases $\bm  {\theta }$ to parameters $\mathbf  {z} = f_{\bm  {\theta }}(\mathbf  {w})$ distributed as $q_{\bm  {\theta }}(\mathbf  {z} \mid \mathcal  {X})$. B. The EPI distribution of STG model parameters producing network syncing. Samples are colored by log probability density. Distribution contours of emergent property value error are shown at levels of $2.5 \times 10^{-5}$, $5 \times 10^{-5}$, $1 \times 10^{-4}$, $2 \times 10^{-4}$, and $4 \times 10^{-4}$ (dark to light gray). Eigenvectors of the Hessian at the mode of the inferred distribution are indicated as $\mathbf  {v}_1$ (solid) and $\mathbf  {v}_2$ (dashed) with lengths scaled by the square root of the absolute value of their eigenvalues. Simulated activity is shown for three samples (stars)in panel D. $v_1$ is sensitive to network syncing ($p<10^{-4}$), while $v_2$ is not ($p=0.67$) (see Section 5.2.1\hbox {}). D. The emergent property of unified intermediacy, in which all neurons are firing close to the same intermediate frequency. Simulated activity traces are colored by log probability density of their generating parameters in the EPI-inferred distribution. }}{6}\protected@file@percent }
\newlabel{fig:STG}{{1}{6}}
\citation{rezende2015variational}
\citation{dinh2016density}
\citation{papamakarios2017masked}
\citation{jaynes1957information}
\citation{elsayed2017structure}
\citation{loaiza2017maximum}
\citation{savin2017maximum}
\newlabel{eq:EP}{{2}{7}}
\newlabel{eq:EPI}{{3}{7}}
\citation{tsodyks1997paradoxical}
\citation{murphy2009balanced}
\citation{ozeki2009inhibitory}
\citation{rubin2015stabilized}
\citation{markram2004interneurons}
\citation{rudy2011three}
\citation{tremblay2016}
\citation{pfeffer2013inhibition}
\citation{litwin2016inhibitory}
\citation{GarciaDelMolino2017}
\citation{Chen2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Comprehensive input-responsivity in a nonlinear sensory system}{8}\protected@file@percent }
\newlabel{results_V1}{{3.3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \small  Hypothesis generation through EPI in a V1 model. A. Four-population model of primary visual cortex with excitatory (black), parvalbumin (blue), somatostatin (red), and VIP (green) neurons. Some neuron-types largely do not form synaptic projections to others (excitatory and inhibitory projections filled and unfilled, respectively). B. Linear response predictions become inaccurate with greater input strength. V1 model simulations for input (solid) $h=b$ and (dashed) $h = b + dh$. Stars indicate the linear response prediction. C. EPI distributions on differential input $dh$ conditioned on differential response $\mathcal  {B}(\alpha , y)$. Supporting evidence for the four generated hypotheses are indicated by gray boxes with labels H1, H2, H3, and H4. The linear prediction from two standard deviations away from $y$ (from negative to positive) is overlaid in magenta (very small, near origin). }}{9}\protected@file@percent }
\newlabel{fig:V1_EPI}{{2}{9}}
\citation{allen2018layer}
\citation{billeh2019systematic}
\citation{duan2015requirement}
\citation{duan2018collicular}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Confirming EPI generated hypotheses in V1. A. Differential responses $\delta x_E$ by the E-population to changes in individual input $\delta h_\alpha \hat  {u}_\alpha $ away from the mode of the EPI distribution $dh^*$. B-D Same plots for the P-, S-, and V-populations. Labels H1, H2, H3, and H4 indicate which curves confirm which hypotheses.}}{12}\protected@file@percent }
\newlabel{fig:V1_HT}{{3}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Identifying neural mechanisms of flexible task switching}{12}\protected@file@percent }
\newlabel{results_SC}{{3.4}{12}}
\citation{duan2018collicular}
\citation{duan2018collicular}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \small  EPI reveals changes in SC \cite  {duan2018collicular} connectivity that control task accuracy. A. Rapid task switching behavioral paradigm (see text). B. Model of superior colliculus (SC). Neurons: LP - left pro, RP - right pro, LA - left anti, RA - right anti. Parameters: $sW$ - self, $hW$ - horizontal, $vW$ -vertical, $dW$ - diagonal weights. Subscripts $P$ and $A$ of connectivity weights indicate Pro or Anti populations, and e.g. $vW_{PA}$ is a vertical weight from an Anti to a Pro population. C. The Schur decomposition of the weight matrix $W = V\Lambda V^{-1}$ is a unique decomposition with orthogonal $V$ and upper triangular $\Lambda $. Schur modes: $v_{\text  {all}}$, $v_{\text  {task}}$, $v_{\text  {side}}$, and $v_{\text  {diag}}$. D. The marginal EPI distributions of the Schur eigenvalues at each level of task accuracy. E. The correlation of Schur eigenvalue with task performance in each learned EPI distribution.}}{14}\protected@file@percent }
\newlabel{fig:SC}{{4}{14}}
\citation{barak2017recurrent}
\citation{sussillo2013opening}
\citation{mastrogiuseppe2018linking}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Linking RNN connectivity to error}{16}\protected@file@percent }
\newlabel{results_RNN}{{3.5}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \small  Sources of error in an RNN solving a simple task. A. (left) A rank-1 RNN executing a Gaussian posterior conditioning computation on $\mu _y$. (right) Error in this computation can come from over- or under-estimating the posterior mean or variance. B. EPI distribution of rank-1 RNNs executing Gaussian posterior conditioning. Samples are colored by (left) posterior mean $\mu _{\text  {post}}=\kappa _r$ and (right) posterior variance $\sigma ^2_{\text  {post}}=\Delta _T$ C. Finite-size network simulations of 2,000 neurons with parameters $z_1$ and $z_2$ sampled from the inferred distribution. Activity along readout $\kappa _r$ (cyan) is stable despite chaotic fluctuations. D. The posterior mean computed by RNNs parameterized by $z_1$ and $z_2$ perturbed in the dimension of the product of $M_m$ and $M_n$. Means and standard errors are shown across 10 realizations of 2,000-neuron networks.}}{17}\protected@file@percent }
\newlabel{fig:RNN}{{5}{17}}
\citation{mastrogiuseppe2018linking}
\citation{sompolinsky1988chaos}
\citation{mastrogiuseppe2018linking}
\citation{loaiza2017maximum}
\citation{mastrogiuseppe2018linking}
\citation{kass2001spike}
\citation{brown1998statistical}
\citation{paninski2004maximum}
\citation{truccolo2005point}
\citation{druckmann2007novel}
\citation{byron2009gaussian}
\citation{park2011bayesian}
\citation{latimer2015single}
\citation{lakshminarasimhan2018dynamic}
\citation{duncker2019learning}
\citation{ladenbauer2019inferring}
\citation{paninski2018neural}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}EPI is a general tool for theoretical neuroscience}{19}\protected@file@percent }
\citation{kraynyukova2018stabilized}
\citation{morrison2016diversity}
\citation{litwin2016inhibitory}
\citation{GarciaDelMolino2017}
\citation{Chen2019}
\citation{pitkow2017inference}
\citation{echeveste2019cortical}
\citation{mastrogiuseppe2018linking}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Novel hypotheses from EPI}{20}\protected@file@percent }
\citation{richards2019deep}
\bibdata{eLife2020}
\bibcite{abbott2008theoretical}{1}
\bibcite{hopfield1982neural}{2}
\bibcite{sompolinsky1988chaos}{3}
\bibcite{tsodyks1997paradoxical}{4}
\bibcite{wong2006recurrent}{5}
\bibcite{liepe2014framework}{6}
\bibcite{kingma2013auto}{7}
\bibcite{rezende2014stochastic}{8}
\bibcite{gao2016linear}{9}
\bibcite{zhao2017recursive}{10}
\bibcite{barello2018sparse}{11}
\bibcite{pandarinath2018inferring}{12}
\bibcite{wiltschko2015mapping}{13}
\bibcite{johnson2016composing}{14}
\bibcite{batty2019behavenet}{15}
\bibcite{paninski2018neural}{16}
\bibcite{raue2009structural}{17}
\bibcite{gelman2013philosophy}{18}
\bibcite{blei2014build}{19}
\bibcite{transtrum2015perspective}{20}
\bibcite{tran2017hierarchical}{21}
\bibcite{rezende2015variational}{22}
\bibcite{dinh2016density}{23}
\bibcite{papamakarios2017masked}{24}
\bibcite{loaiza2017maximum}{25}
\bibcite{goldman2001global}{26}
\bibcite{gutierrez2013multiple}{27}
\bibcite{litwin2016inhibitory}{28}
\bibcite{duan2018collicular}{29}
\bibcite{mastrogiuseppe2018linking}{30}
\bibcite{marder2002cellular}{31}
\bibcite{prinz2004similar}{32}
\bibcite{jaynes1957information}{33}
\bibcite{elsayed2017structure}{34}
\bibcite{savin2017maximum}{35}
\bibcite{murphy2009balanced}{36}
\bibcite{ozeki2009inhibitory}{37}
\bibcite{rubin2015stabilized}{38}
\bibcite{markram2004interneurons}{39}
\bibcite{rudy2011three}{40}
\bibcite{tremblay2016}{41}
\bibcite{pfeffer2013inhibition}{42}
\bibcite{GarciaDelMolino2017}{43}
\bibcite{Chen2019}{44}
\bibcite{allen2018layer}{45}
\bibcite{billeh2019systematic}{46}
\bibcite{duan2015requirement}{47}
\bibcite{barak2017recurrent}{48}
\bibcite{sussillo2013opening}{49}
\bibcite{kass2001spike}{50}
\bibcite{brown1998statistical}{51}
\bibcite{paninski2004maximum}{52}
\bibcite{truccolo2005point}{53}
\bibcite{druckmann2007novel}{54}
\bibcite{byron2009gaussian}{55}
\bibcite{park2011bayesian}{56}
\bibcite{latimer2015single}{57}
\bibcite{lakshminarasimhan2018dynamic}{58}
\bibcite{duncker2019learning}{59}
\bibcite{ladenbauer2019inferring}{60}
\bibcite{kraynyukova2018stabilized}{61}
\bibcite{morrison2016diversity}{62}
\bibcite{pitkow2017inference}{63}
\bibcite{echeveste2019cortical}{64}
\bibcite{richards2019deep}{65}
\bibcite{blei2017variational}{66}
\bibcite{ranganath2014black}{67}
\bibcite{bittner2019degenerate}{68}
\bibcite{bittner2019examining}{69}
\bibcite{nonnenmacher2018sbi}{70}
\bibcite{desitler2019statistical}{71}
\bibcite{gonccalves2019training}{72}
\bibcite{LueckmannGoncalves_17}{73}
\bibcite{wainwright2008graphical}{74}
\bibcite{kingma2014adam}{75}
\bibcite{dinh2017density}{76}
\bibcite{brunel2000dynamics}{77}
\bibcite{jaeger2004harnessing}{78}
\bibcite{sussillo2009generating}{79}
\bibstyle{unsrt}
\citation{loaiza2017maximum}
\citation{tran2017hierarchical}
\@writefile{toc}{\contentsline {section}{\numberline {5}Methods}{30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Emergent property inference (EPI)}{30}\protected@file@percent }
\newlabel{methods_EPI}{{5.1}{30}}
\newlabel{eq:opt}{{17}{30}}
\citation{blei2017variational}
\citation{kingma2013auto}
\citation{ranganath2014black}
\citation{tran2017hierarchical}
\citation{bittner2019degenerate}
\citation{bittner2019examining}
\citation{nonnenmacher2018sbi}
\citation{desitler2019statistical}
\citation{gonccalves2019training}
\citation{loaiza2017maximum}
\citation{LueckmannGoncalves_17}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Example: 2D LDS}{31}\protected@file@percent }
\newlabel{methods_2DLDS}{{5.1.1}{31}}
\citation{wainwright2008graphical}
\citation{kingma2014adam}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Augmented Lagrangian optimization}{35}\protected@file@percent }
\newlabel{methods_AL_opt}{{5.1.2}{35}}
\newlabel{eq:AL}{{26}{35}}
\citation{rezende2015variational}
\citation{rezende2015variational}
\citation{dinh2017density}
\citation{blei2017variational}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Normalizing flows}{37}\protected@file@percent }
\newlabel{methods_NF}{{5.1.3}{37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Emergent property inference as variational inference in an exponential family}{37}\protected@file@percent }
\newlabel{methods_VI}{{5.1.4}{37}}
\citation{wainwright2008graphical}
\citation{blei2017variational}
\citation{gutierrez2013multiple}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Theoretical models}{39}\protected@file@percent }
\newlabel{methods_theoretical_models}{{5.2}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Stomatogastric ganglion}{39}\protected@file@percent }
\newlabel{methods_STG}{{5.2.1}{39}}
\citation{billeh2019systematic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Primary visual cortex}{43}\protected@file@percent }
\newlabel{methods_V1}{{5.2.2}{43}}
\citation{litwin2016inhibitory}
\citation{GarciaDelMolino2017}
\citation{Chen2019}
\newlabel{eq:V1_W}{{61}{44}}
\citation{duan2018collicular}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Superior colliculus}{46}\protected@file@percent }
\newlabel{methods_SC}{{5.2.3}{46}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \small  Table of significant correlation values from Fig. 4\hbox {}E.}}{49}\protected@file@percent }
\citation{sompolinsky1988chaos}
\citation{brunel2000dynamics}
\citation{jaeger2004harnessing}
\citation{sussillo2009generating}
\citation{sompolinsky1988chaos}
\citation{mastrogiuseppe2018linking}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Rank-1 RNN}{50}\protected@file@percent }
\newlabel{methods_LRRNN}{{5.2.4}{50}}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
